{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633a9b94",
   "metadata": {},
   "source": [
    "# LLM Metrics Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7405f3",
   "metadata": {},
   "source": [
    "### NOTES:\n",
    "- I want to know if there is a difference between one or 4 GPU’s. How can we measure if we need a 70b or a 7b.\n",
    "- Brevity is pretty high of interest for things. We want to know if something will take folks a long time to read through.\n",
    "- Model choices, Mistral, Llama, Mistral (70B) - hugging face things.\n",
    "- I also want to know basic performance metrics about compute and latency across the models.\n",
    "- Make preprocessing scripts using the GitHub repo with dummy PDF data so that it fulfills the requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30b967",
   "metadata": {},
   "source": [
    "### NOTES 2:\n",
    "1. Latency\n",
    "    1. Inference Warmup Time: Time required for the model to stabilize during the warm-up phase.\n",
    "2. Power Consumption\n",
    "    1. Energy Consumption Per Token: Power consumption normalized by the number of tokens generated.\n",
    "3. Batch Scaling\n",
    "4. Precision Comparison\n",
    "    1. Precision Accuracy: Compare text output differences for fp32 vs. fp16 to ensure precision integrity.\n",
    "5. Memory Usage by Sequence Length\n",
    "6. Token Throughput: Number of tokens processed per second.\n",
    "7. Model Size (Parameter Count): Display the total number of trainable parameters in the model.\n",
    "8. GPU Utilization: Percentage of GPU compute capacity used during inference.\n",
    "9. GPU Memory Used: Peak GPU memory consumed during inference (in GB).\n",
    "10. Response Length and Brevity Evaluation: Evaluate response length and calculate brevity ratio (output length / input length)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5e43f",
   "metadata": {},
   "source": [
    "## Implemented for hugging face metric evaluation currently.\n",
    "- BLEU (Bilingual Evaluation Understudy)\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- METEOR\n",
    "- BERTScore\n",
    "- RAGAS (Retrieval-Augmented Generation Answer Score)\n",
    "- HELM (Holistic Evaluation of Language Models)\n",
    "- GPT-Score\n",
    "- Scenario Fidelity Tests\n",
    "- Forgetting Rate\n",
    "- RIMU (Relevance, Integration, Memory, and Usefulness)\n",
    "- Problem-Solving Effectiveness\n",
    "\n",
    "\n",
    "## Ones to consider for later.\n",
    "- F1 for fact matching\n",
    "- Context Coverage Score\n",
    "- Memory Application Score\n",
    "- Utility in Open-Ended Tasks\n",
    "- Temporal Utility Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234f1c8",
   "metadata": {},
   "source": [
    "### DRAFT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b68ca",
   "metadata": {},
   "source": [
    "LLM Metrics Resources List:\n",
    "\n",
    "Answer Relevance and Correctness\n",
    "Metrics:\n",
    "BLEU (Bilingual Evaluation Understudy): Measures n-gram overlap for syntactic similarity.\n",
    "GitHub: BLEU Implementation\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation): Focuses on recall for summarization tasks.\n",
    "GitHub: ROUGE Google Research\n",
    "METEOR: Combines precision, recall, stemming, and synonym matching for semantic similarity.\n",
    "Paper: METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments\n",
    "BERTScore: Uses embeddings from BERT or similar models to evaluate semantic similarity.\n",
    "GitHub: BERTScore Implementation\n",
    "RAGAS (Retrieval-Augmented Generation Answer Score): Evaluates retrieval-augmented responses based on relevance, groundedness, and informativeness.\n",
    "GitHub: RAGAS Implementation\n",
    "F1 for Fact Matching: Measures exact or partial match of factual elements.\n",
    "GitHub: Scikit-Learn F1 Metric\n",
    "\n",
    "Context Handling and Memory\n",
    "Metrics:\n",
    "Context Coverage Score: Measures the proportion of relevant context elements used in the response.\n",
    "No direct implementation; typically evaluated with custom scripts.\n",
    "Memory Application Score: Tests the integration of past context or facts into responses.\n",
    "Custom metric based on embeddings or human feedback.\n",
    "Cross-Session Recall: Evaluates the ability to retain context across interactions.\n",
    "No direct implementation; scenario-based evaluation required.\n",
    "FEQA (Faithfulness QA): Measures faithfulness of generated text to its source context.\n",
    "GitHub: FEQA Implementation\n",
    "QAGS (Question-Answering and Generation): Assesses factual consistency using QA systems.\n",
    "GitHub: QAGS Implementation\n",
    "\n",
    "Factuality and Faithfulness\n",
    "Metrics:\n",
    "Groundedness Score: Evaluates factual accuracy and grounding in provided evidence.\n",
    "Typically custom implementations using external knowledge bases.\n",
    "TruthfulQA: Measures truthfulness and resistance to generating false information.\n",
    "GitHub: TruthfulQA Benchmark\n",
    "Faithfulness Metrics (e.g., FEQA, QAGS): See Context Handling.\n",
    "\n",
    "Utility and Task Effectiveness\n",
    "Metrics:\n",
    "Task Completion Rate: Measures success in achieving user-defined tasks.\n",
    "No specific implementation; commonly measured via user testing.\n",
    "Response Actionability: Tests whether the response provides actionable and useful next steps.\n",
    "Custom metric, often integrated into manual evaluations.\n",
    "HELM (Holistic Evaluation of Language Models): Evaluates accuracy, robustness, efficiency, and other dimensions.\n",
    "Website: HELM Overview\n",
    "HELP (Human Evaluation of Language Processing): Rates correctness, relevance, coherence, and grammaticality.\n",
    "Paper: HELP Framework\n",
    "\n",
    "Engagement and Naturalness\n",
    "Metrics:\n",
    "Conversational Usefulness: Evaluates the ability to maintain engagement while being task-oriented.\n",
    "Custom metric based on human evaluation.\n",
    "Empathy and Alignment Score: Tests alignment with the user’s emotional tone and conversational needs.\n",
    "No specific implementation available.\n",
    "GPT-Score: Uses a secondary LLM to evaluate fluency, coherence, and naturalness.\n",
    "GitHub: GPT-Score Implementation\n",
    "\n",
    "Robustness and Adaptability\n",
    "Metrics:\n",
    "Ambiguity Resolution Score: Evaluates the ability to clarify ambiguous inputs.\n",
    "No specific implementation available.\n",
    "Conflict Resolution Score: Assesses reconciliation of conflicting information in context.\n",
    "Custom evaluations needed.\n",
    "Scenario Fidelity Tests: Simulate complex user scenarios to test robustness.\n",
    "Custom scripts or manually crafted scenarios.\n",
    "\n",
    "Efficiency and Scalability\n",
    "Metrics:\n",
    "Latency Metrics: Measures response time for varied query complexities.\n",
    "No specific implementation available; commonly logged in production systems.\n",
    "Context Window Efficiency: Assesses how efficiently key context elements are prioritized in limited input token space.\n",
    "Often integrated into custom testing setups.\n",
    "OpenAI’s Evaluation Framework: Provides tools for automated and manual evaluation of various metrics.\n",
    "GitHub: OpenAI Evaluations\n",
    "\n",
    "Tools and Frameworks\n",
    "Integrated Evaluation Toolkits:\n",
    "Hugging Face Datasets and Metrics: Provides a unified library for BLEU, ROUGE, and other metrics.\n",
    "Website: Hugging Face Metrics\n",
    "RAGAS for Retrieval-Augmented Models: Specifically for retrieval-augmented systems.\n",
    "GitHub: RAGAS Implementation\n",
    "TruthfulQA Benchmark: Focused on truthfulness in language models.\n",
    "GitHub: TruthfulQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfaf0a8",
   "metadata": {},
   "source": [
    "### Appendix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c3a70",
   "metadata": {},
   "source": [
    "Answer Usefulness and Utility\n",
    "Task Completion Rate: Measures whether the response effectively completes the user’s request or task, especially for goal-oriented interactions (e.g., booking tickets, solving a problem).\n",
    "Response Actionability Score: Evaluates whether the response provides clear, actionable information or next steps that are useful to the user.\n",
    "Specificity and Relevance: Measures how tailored the response is to the query, avoiding vague or overly general answers.\n",
    "Utility Feedback: Gathers user feedback to rate the usefulness of responses on a Likert scale (e.g., 1-5).\n",
    "Information Coverage: Tests if the response sufficiently covers the query while avoiding unnecessary details or omissions.\n",
    "Error Reduction Rate: Evaluates how often the LLM provides responses that prevent misunderstandings, errors, or missteps in task execution.\n",
    "\n",
    "Contextual Relevance and Use\n",
    "Context Coverage Score: Measures the percentage of relevant contextual information used in generating the response.\n",
    "Attention Consistency: Assesses whether the model focuses on and references the correct parts of the input context during inference.\n",
    "Sequential Context Utility: Evaluates how well the model utilizes information from multiple prior turns to maintain relevance and coherence.\n",
    "Topic Continuity Score: Measures the ability of the model to stay on topic across multi-turn dialogues without unnecessary drift.\n",
    "Ambiguity Resolution Score: Assesses the model’s ability to disambiguate unclear queries using context and memory effectively.\n",
    "\n",
    "Memory Handling and Long-Term Usefulness\n",
    "Temporal Recall Accuracy: Measures whether the model recalls key facts or events over time, distinguishing between short-term and long-term memory.\n",
    "Memory Relevance Metric: Assesses whether recalled information is relevant to the current query and used appropriately.\n",
    "Forgetting Rate: Tracks how quickly the model loses the ability to recall and apply previously introduced information.\n",
    "Memory Application Score: Measures the ability to integrate past knowledge to provide deeper, more informed responses.\n",
    "Cross-Session Recall: Tests how well the model retains and applies relevant information from earlier sessions.\n",
    "\n",
    "Factuality and Faithfulness\n",
    "Groundedness Score: Evaluates whether the response is factually accurate and supported by the input context or retrieved documents (in RAG systems).\n",
    "Faithfulness to Source: Assesses whether the response stays true to provided data without introducing hallucinations.\n",
    "Factual Usefulness: Measures whether factual information provided is not only correct but also pertinent to the user’s needs.\n",
    "Error Propagation Metric: Tracks whether the model compounds errors by misapplying incorrect or irrelevant facts.\n",
    "\n",
    "Response Efficiency and Practicality\n",
    "Response Brevity Score: Evaluates whether the response is concise yet complete, avoiding excessive verbosity.\n",
    "Clarity Index: Measures the ease with which a user can understand and apply the response.\n",
    "Redundancy Rate: Tracks unnecessary repetition or over-elaboration in responses.\n",
    "Latency in Task Resolution: Measures the time it takes for a user to resolve their query with the model's help.\n",
    "\n",
    "Engagement and Human-Like Interaction\n",
    "Conversational Usefulness: Evaluates whether the interaction maintains engagement while being task-oriented and useful.\n",
    "Empathy and Alignment Score: Measures how well the model aligns with the user’s emotional tone and conversational needs.\n",
    "Dialogue Flow Score: Assesses whether responses contribute to a seamless, natural interaction without jarring transitions.\n",
    "Personalization Effectiveness: Evaluates how well the model adapts its responses to the user’s individual preferences or prior interactions.\n",
    "\n",
    "Robustness Across Complex Scenarios\n",
    "Scenario-Specific Usefulness: Tests response utility in specialized scenarios, such as medical advice, legal queries, or customer support.\n",
    "Context Switching Efficiency: Evaluates the model’s ability to handle sudden topic changes without confusion or loss of relevance.\n",
    "Conflict Resolution Score: Assesses the ability to reconcile contradictory information in context while maintaining a useful response.\n",
    "Multi-Faceted Query Handling: Measures effectiveness in addressing queries that involve multiple sub-tasks or dimensions.\n",
    "\n",
    "Scenario-Based Composite Metrics\n",
    "Utility in Open-Ended Tasks: Evaluates usefulness in creative or exploratory queries, such as brainstorming or storytelling.\n",
    "Problem-Solving Effectiveness: Measures how well the model aids in solving logical, mathematical, or domain-specific problems.\n",
    "Instruction Following Accuracy: Tracks adherence to complex user instructions while ensuring utility in the output.\n",
    "\n",
    "Evaluation Frameworks for Usefulness\n",
    "Integrated Metrics\n",
    "RIMU (Relevance, Integration, Memory, and Usefulness): Combines relevance, memory effectiveness, and user-centric utility into a composite score.\n",
    "Actionability and Recall Score (ARS): Evaluates a response based on how actionable and contextually grounded it is while utilizing memory effectively.\n",
    "Temporal Utility Metric (TUM): Measures response utility over time, assessing how context and memory are applied in long-term interactions.\n",
    "Human and Simulated Feedback\n",
    "Real-User Task Evaluation: Have real users evaluate response usefulness in practical tasks and rate task success.\n",
    "Simulated Scenarios: Test model responses in carefully crafted, multi-turn scenarios with defined utility goals.\n",
    "Crowdsourced Usefulness Scores: Use human annotators to provide quantitative scores for response utility.\n",
    "Automated Testing\n",
    "Probing for Utility: Use automated tools to systematically test the model’s ability to produce useful responses across different task types.\n",
    "Retrieval-Augmented Benchmarks: Evaluate response quality in RAG systems by testing the relationship between retrieved evidence and its application in generating useful answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f80d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
