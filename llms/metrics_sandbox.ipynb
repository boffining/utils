{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49e166e",
   "metadata": {},
   "source": [
    "## Sandbox for LLM Metrics Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f00e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b78cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233451fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dac235b7",
   "metadata": {},
   "source": [
    "## report.py with graph outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b128e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any, List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import (\n",
    "    evaluate_latency_throughput,\n",
    "    evaluate_power_efficiency,\n",
    "    compare_precision_accuracy,\n",
    "    memory_by_sequence_length,\n",
    ")\n",
    "from metrics import (\n",
    "    calculate_perplexity,\n",
    "    calculate_f1_score,\n",
    "    calculate_precision_recall,\n",
    "    calculate_mean_reciprocal_rank,\n",
    "    calculate_mean_average_precision,\n",
    ")\n",
    "from pynvml import nvmlInit, nvmlDeviceGetCount, nvmlDeviceGetHandleByIndex, nvmlDeviceGetName\n",
    "\n",
    "\n",
    "def get_hardware_info() -> dict:\n",
    "    \"\"\"\n",
    "    Gather hardware details about the system used for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the number of GPUs, GPU types, and whether CUDA is available.\n",
    "    \"\"\"\n",
    "    nvmlInit()\n",
    "    gpu_count = nvmlDeviceGetCount()\n",
    "    gpu_info = [nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(i)).decode() for i in range(gpu_count)]\n",
    "    return {\n",
    "        \"num_gpus\": gpu_count,\n",
    "        \"gpu_types\": gpu_info,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_graphs(report: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Generate graphs for the evaluation results.\n",
    "\n",
    "    Args:\n",
    "        report (Dict[str, Any]): The evaluation report containing metrics and results.\n",
    "    \"\"\"\n",
    "    # Latency and throughput\n",
    "    if \"latency_throughput\" in report[\"evaluation_results\"]:\n",
    "        results = report[\"evaluation_results\"][\"latency_throughput\"]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar([\"Latency\", \"Throughput\", \"Token Throughput\"], [results[\"latency\"], results[\"throughput\"], results[\"token_throughput\"]])\n",
    "        plt.title(\"Latency and Throughput\")\n",
    "        plt.ylabel(\"Time/Throughput\")\n",
    "        plt.savefig(\"latency_throughput.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Power efficiency\n",
    "    if \"power_efficiency\" in report[\"evaluation_results\"]:\n",
    "        results = report[\"evaluation_results\"][\"power_efficiency\"]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar([\"Power Consumed\", \"Energy per Token\"], [results[\"power_consumed\"], results[\"energy_per_token\"]])\n",
    "        plt.title(\"Power Efficiency\")\n",
    "        plt.ylabel(\"Watts/Energy\")\n",
    "        plt.savefig(\"power_efficiency.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Memory usage by sequence length\n",
    "    if \"memory_usage\" in report[\"evaluation_results\"]:\n",
    "        memory_results = report[\"evaluation_results\"][\"memory_usage\"]\n",
    "        sequence_lengths = list(memory_results.keys())\n",
    "        memory_usages = list(memory_results.values())\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(sequence_lengths, memory_usages, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "        plt.title(\"Memory Usage by Sequence Length\")\n",
    "        plt.xlabel(\"Sequence Length\")\n",
    "        plt.ylabel(\"Memory Usage (MB)\")\n",
    "        plt.grid()\n",
    "        plt.savefig(\"memory_usage.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Metrics summary\n",
    "    if \"metrics\" in report[\"evaluation_results\"]:\n",
    "        metrics = report[\"evaluation_results\"][\"metrics\"]\n",
    "        names = list(metrics.keys())\n",
    "        values = [metrics[name] if isinstance(metrics[name], (int, float)) else 0 for name in names]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(names, values, color=\"g\")\n",
    "        plt.title(\"Metrics Summary\")\n",
    "        plt.ylabel(\"Scores\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.savefig(\"metrics_summary.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model_name: str,\n",
    "    evaluate_latency: bool = True,\n",
    "    evaluate_power: bool = True,\n",
    "    evaluate_precision: bool = True,\n",
    "    evaluate_memory: bool = True,\n",
    "    evaluate_metrics: bool = True,\n",
    "    quantized: bool = False,\n",
    "    prompt: str = \"What is the impact of climate change?\",\n",
    "    batch_size: int = 4,\n",
    "    max_tokens: int = 50,\n",
    "    sequence_lengths: List[int] = [128, 256, 512, 1024],\n",
    "    relevance_scores: List[List[int]] = [[1, 0, 1, 1, 0]],\n",
    "    probabilities: List[float] = [0.2, 0.3, 0.1, 0.4],\n",
    "    ranks: List[int] = [1, 3, 2, 0],\n",
    "    precision: float = 0.8,\n",
    "    recall: float = 0.75,\n",
    "    true_positive: int = 50,\n",
    "    false_positive: int = 10,\n",
    "    false_negative: int = 15,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate an LLM using specified metrics and utilities.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The Hugging Face model name or path.\n",
    "        evaluate_latency (bool): Whether to evaluate latency and throughput.\n",
    "        evaluate_power (bool): Whether to evaluate power efficiency.\n",
    "        evaluate_precision (bool): Whether to compare precision between fp32 and fp16.\n",
    "        evaluate_memory (bool): Whether to measure memory usage for varying sequence lengths.\n",
    "        evaluate_metrics (bool): Whether to compute various metrics like perplexity, F1 score, etc.\n",
    "        quantized (bool): Whether the model is quantized or not.\n",
    "        prompt (str): The input prompt for the model.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        max_tokens (int): Maximum tokens to generate.\n",
    "        sequence_lengths (List[int]): List of sequence lengths for memory evaluation.\n",
    "        relevance_scores (List[List[int]]): Relevance scores for MAP calculation.\n",
    "        probabilities (List[float]): Probabilities for perplexity calculation.\n",
    "        ranks (List[int]): Ranks for MRR calculation.\n",
    "        precision (float): Precision score for F1 calculation.\n",
    "        recall (float): Recall score for F1 calculation.\n",
    "        true_positive (int): True positives for precision/recall calculation.\n",
    "        false_positive (int): False positives for precision/recall calculation.\n",
    "        false_negative (int): False negatives for precision/recall calculation.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A detailed report of all evaluations performed.\n",
    "    \"\"\"\n",
    "    report = {\"model_name\": model_name, \"evaluation_results\": {}, \"conditions\": {}}\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Hardware information\n",
    "    hardware_info = get_hardware_info()\n",
    "    model_parameters = sum(p.numel() for p in model.parameters())\n",
    "    report[\"conditions\"] = {\n",
    "        \"prompt\": prompt,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"sequence_lengths\": sequence_lengths,\n",
    "        \"num_gpus\": hardware_info[\"num_gpus\"],\n",
    "        \"gpu_types\": hardware_info[\"gpu_types\"],\n",
    "        \"cuda_available\": hardware_info[\"cuda_available\"],\n",
    "        \"model_parameters\": model_parameters,\n",
    "        \"quantized\": quantized,\n",
    "    }\n",
    "\n",
    "    # Perform evaluations\n",
    "    if evaluate_latency:\n",
    "        outputs, latency, throughput, token_throughput = evaluate_latency_throughput(\n",
    "            model, tokenizer, prompt, max_tokens, batch_size\n",
    "        )\n",
    "        report[\"evaluation_results\"][\"latency_throughput\"] = {\n",
    "            \"latency\": latency,\n",
    "            \"throughput\": throughput,\n",
    "            \"token_throughput\": token_throughput,\n",
    "        }\n",
    "\n",
    "    if evaluate_power:\n",
    "        power_consumed, energy_per_token = evaluate_power_efficiency(\n",
    "            model, tokenizer, prompt, max_tokens, batch_size\n",
    "        )\n",
    "        report[\"evaluation_results\"][\"power_efficiency\"] = {\n",
    "            \"power_consumed\": power_consumed,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "        }\n",
    "\n",
    "    if evaluate_precision:\n",
    "        precision_match = compare_precision_accuracy(model_name, prompt, max_tokens)\n",
    "        report[\"evaluation_results\"][\"precision_comparison\"] = {\"precision_match\": precision_match}\n",
    "\n",
    "    if evaluate_memory:\n",
    "        memory_results = {}\n",
    "        for length in sequence_lengths:\n",
    "            memory_usage = memory_by_sequence_length(model, tokenizer, prompt, max_tokens, length)\n",
    "            memory_results[f\"sequence_length_{length}\"] = memory_usage\n",
    "        report[\"evaluation_results\"][\"memory_usage\"] = memory_results\n",
    "\n",
    "    if evaluate_metrics:\n",
    "        metrics_results = {\n",
    "            \"perplexity\": calculate_perplexity(probabilities),\n",
    "            \"f1_score\": calculate_f1_score(precision, recall),\n",
    "            \"precision_recall\": calculate_precision_recall(true_positive, false_positive, false_negative),\n",
    "            \"mrr\": calculate_mean_reciprocal_rank(ranks),\n",
    "            \"map\": calculate_mean_average_precision(relevance_scores),\n",
    "        }\n",
    "        report[\"evaluation_results\"][\"metrics\"] = metrics_results\n",
    "\n",
    "    # Generate graphs for the report\n",
    "    generate_graphs(report)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "    evaluation_report = evaluate_model(\n",
    "        model_name,\n",
    "        evaluate_latency=True,\n",
    "        evaluate_power=True,\n",
    "        evaluate_precision=True,\n",
    "        evaluate_memory=True,\n",
    "        evaluate_metrics=True,\n",
    "        quantized=False,\n",
    "    )\n",
    "    print(\"\\nEvaluation Report:\")\n",
    "    print(evaluation_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50558172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "utils.py\n",
    "\n",
    "This module contains utilities for evaluating the performance of large language models (LLMs) using\n",
    "metrics such as latency, throughput, power consumption, memory usage, and precision comparison.\n",
    "It is designed for use with Hugging Face's open-source LLMs, including models like Llama 2 7B.\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- torch\n",
    "- pynvml\n",
    "\n",
    "Install dependencies:\n",
    "    pip install transformers torch pynvml\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetPowerUsage, nvmlDeviceGetMemoryInfo\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Initialize NVIDIA Management Library\n",
    "nvmlInit()\n",
    "gpu_handle = nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "# Helper functions\n",
    "def track_power() -> float:\n",
    "    \"\"\"Track GPU power consumption in watts.\"\"\"\n",
    "    return nvmlDeviceGetPowerUsage(gpu_handle) / 1000\n",
    "\n",
    "def track_memory() -> float:\n",
    "    \"\"\"Track GPU memory usage in GB.\"\"\"\n",
    "    mem_info = nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    return mem_info.used / (1024 ** 3)\n",
    "\n",
    "def count_model_parameters(model: torch.nn.Module) -> int:\n",
    "    \"\"\"Count the total number of parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "class LLMPerformanceTester:\n",
    "    \"\"\"\n",
    "    A utility class for evaluating the performance of Hugging Face LLMs.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): The Hugging Face model name to load.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        model: The LLM model loaded from Hugging Face.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        parameter_count (int): Number of parameters in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.parameter_count = count_model_parameters(self.model)\n",
    "        print(f\"Model Size: {self.parameter_count:,} parameters\")\n",
    "\n",
    "    def evaluate_latency_throughput(self, prompt: str, max_tokens: int = 50, batch_size: int = 1) -> Tuple[torch.Tensor, float, float, float]:\n",
    "        \"\"\"\n",
    "        Evaluate latency, token throughput, and token processing rate.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt for the model.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "            batch_size (int): Number of prompts in a batch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, float, float, float]: Generated outputs, latency, throughput, and token throughput.\n",
    "        \"\"\"\n",
    "        inputs = [prompt] * batch_size\n",
    "        tokenized_inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "\n",
    "        # Warm-up\n",
    "        print(\"Warming up...\")\n",
    "        start_warmup = time.time()\n",
    "        self.model.generate(**tokenized_inputs, max_new_tokens=max_tokens)\n",
    "        end_warmup = time.time()\n",
    "        warmup_time = end_warmup - start_warmup\n",
    "        print(f\"Warm-up Time: {warmup_time:.2f}s\")\n",
    "\n",
    "        # Measure latency and throughput\n",
    "        print(\"Measuring latency and throughput...\")\n",
    "        start_time = time.time()\n",
    "        outputs = self.model.generate(**tokenized_inputs, max_new_tokens=max_tokens)\n",
    "        end_time = time.time()\n",
    "\n",
    "        latency = end_time - start_time\n",
    "        throughput = batch_size / latency\n",
    "        total_tokens = max_tokens * batch_size\n",
    "        token_throughput = total_tokens / latency\n",
    "        print(f\"Latency: {latency:.2f}s | Throughput: {throughput:.2f} responses/sec | Token Throughput: {token_throughput:.2f} tokens/sec\")\n",
    "        return outputs, latency, throughput, token_throughput\n",
    "\n",
    "    def evaluate_power_efficiency(self, prompt: str, max_tokens: int = 50, batch_size: int = 1) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Evaluate power consumption and efficiency per token.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt for the model.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "            batch_size (int): Number of prompts in a batch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Total power consumed and energy per token.\n",
    "        \"\"\"\n",
    "        inputs = [prompt] * batch_size\n",
    "        tokenized_inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "\n",
    "        # Warm-up\n",
    "        print(\"Warming up...\")\n",
    "        self.model.generate(**tokenized_inputs, max_new_tokens=max_tokens)\n",
    "\n",
    "        # Measure power and efficiency\n",
    "        print(\"Measuring power efficiency...\")\n",
    "        power_start = track_power()\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.generate(**tokenized_inputs, max_new_tokens=max_tokens)\n",
    "\n",
    "        end_time = time.time()\n",
    "        power_end = track_power()\n",
    "\n",
    "        latency = end_time - start_time\n",
    "        throughput = batch_size / latency\n",
    "        power_consumed = (power_end - power_start) * latency\n",
    "        total_tokens = max_tokens * batch_size\n",
    "        energy_per_token = power_consumed / total_tokens if total_tokens > 0 else float('inf')\n",
    "\n",
    "        print(f\"Power Consumption: {power_consumed:.2f} W | Energy per Token: {energy_per_token:.4f} W/token\")\n",
    "        return power_consumed, energy_per_token\n",
    "\n",
    "    def compare_precision_accuracy(self, prompt: str, max_tokens: int = 50) -> bool:\n",
    "        \"\"\"\n",
    "        Compare outputs for fp32 and fp16 precision to test integrity.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt for the model.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether the outputs match between fp32 and fp16 precisions.\n",
    "        \"\"\"\n",
    "        # Full precision (fp32)\n",
    "        model_fp32 = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        output_fp32 = model_fp32.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        text_fp32 = self.tokenizer.decode(output_fp32[0], skip_special_tokens=True)\n",
    "\n",
    "        # Mixed precision (fp16)\n",
    "        model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=torch.float16).to(self.device)\n",
    "        output_fp16 = model_fp16.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        text_fp16 = self.tokenizer.decode(output_fp16[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Full Precision (fp32) Output:\\n{text_fp32}\")\n",
    "        print(f\"Mixed Precision (fp16) Output:\\n{text_fp16}\")\n",
    "\n",
    "        similarity = text_fp32 == text_fp16\n",
    "        print(f\"Outputs Match: {similarity}\")\n",
    "        return similarity\n",
    "\n",
    "    def memory_by_sequence_length(self, base_prompt: str, max_tokens: int = 50, max_length: int = 1024) -> None:\n",
    "        \"\"\"\n",
    "        Evaluate memory usage as sequence length increases.\n",
    "\n",
    "        Args:\n",
    "            base_prompt (str): Base string to repeat for increasing sequence length.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "            max_length (int): Maximum sequence length to test.\n",
    "        \"\"\"\n",
    "        print(\"Memory Usage by Sequence Length:\")\n",
    "        for seq_length in [128, 256, 512, max_length]:\n",
    "            prompt = base_prompt * (seq_length // len(base_prompt))\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            self.model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "            print(f\"Sequence Length: {seq_length} | Peak Memory Usage: {peak_memory:.2f} MB\")\n",
    "\n",
    "\"\"\"\n",
    "Usage example:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"meta-llama/Llama-2-7b-hf\"  # Example model\n",
    "    prompt = \"Explain the impact of climate change on global agriculture.\"\n",
    "    base_prompt = \"Climate change affects agriculture in multiple ways. \"\n",
    "\n",
    "    tester = LLMPerformanceTester(model_name)\n",
    "\n",
    "    # Latency and token throughput\n",
    "    tester.evaluate_latency_throughput(prompt, max_tokens=50, batch_size=4)\n",
    "\n",
    "    # Power efficiency and energy per token\n",
    "    tester.evaluate_power_efficiency(prompt, max_tokens=50, batch_size=4)\n",
    "\n",
    "    # Compare fp32 and fp16 outputs\n",
    "    tester.compare_precision_accuracy(prompt, max_tokens=50)\n",
    "\n",
    "    # Memory by sequence length\n",
    "    tester.memory_by_sequence_length(base_prompt, max_tokens=50, max_length=1024)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "metrics.py\n",
    "\n",
    "This module contains implementations and utilities to evaluate language models (LLMs) based on various\n",
    "metrics, including BLEU, ROUGE, METEOR, BERTScore, and others.\n",
    "\n",
    "The metrics are demonstrated with the Hugging Face model \"Mistral\" as an example.\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- torch\n",
    "- datasets\n",
    "- bert_score\n",
    "- nltk\n",
    "\n",
    "Install dependencies:\n",
    "    pip install transformers torch datasets bert-score nltk\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Example Hugging Face model for evaluation\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "def calculate_bleu(predictions: List[str], references: List[List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[List[str]]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: BLEU score and additional metrics.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/huggingface/evaluate\n",
    "    \"\"\"\n",
    "    bleu_metric = load_metric(\"bleu\")\n",
    "    bleu_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = bleu_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_rouge(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: ROUGE scores.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/huggingface/evaluate\n",
    "    \"\"\"\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    rouge_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = rouge_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_meteor(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate METEOR score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: METEOR score.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/huggingface/evaluate\n",
    "    \"\"\"\n",
    "    meteor_metric = load_metric(\"meteor\")\n",
    "    meteor_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = meteor_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_bert_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate BERTScore for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Precision, Recall, and F1 scores.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/Tiiiger/bert_score\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(predictions, references, lang=\"en\", verbose=True)\n",
    "    return {\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()}\n",
    "\n",
    "def calculate_ragas_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate RAGAS (Retrieval-Augmented Generation Answer Score).\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: RAGAS score.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/explodinggradients/ragas\n",
    "    \"\"\"\n",
    "    from ragas import evaluate\n",
    "    ragas_result = evaluate(predictions, references)\n",
    "    return {\"ragas_score\": ragas_result[\"score\"]}\n",
    "\n",
    "def calculate_helm_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate HELM (Holistic Evaluation of Language Models) metrics.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: HELM score.\n",
    "\n",
    "    Resource:\n",
    "        https://crfm.stanford.edu/helm/latest/\n",
    "    \"\"\"\n",
    "    # Placeholder for actual HELM evaluation framework integration\n",
    "    helm_score = np.mean([len(pred) / max(len(ref), 1) for pred, ref in zip(predictions, references)])\n",
    "    return {\"helm_score\": helm_score}\n",
    "\n",
    "def calculate_gpt_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate GPT-Score for text similarity.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: GPT-Score values.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/IntelLabs/gpt-score\n",
    "    \"\"\"\n",
    "    from gpt_score import GPTScorer\n",
    "    scorer = GPTScorer()\n",
    "    scores = scorer.score(predictions, references)\n",
    "    return {\"gpt_score\": np.mean(scores)}\n",
    "\n",
    "def calculate_forgetting_rate(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, prompts: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Forgetting Rate of the model over repeated evaluations.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): The language model to test.\n",
    "        tokenizer (AutoTokenizer): Tokenizer associated with the model.\n",
    "        prompts (List[str]): List of input prompts.\n",
    "\n",
    "    Returns:\n",
    "        float: Forgetting rate as a percentage.\n",
    "\n",
    "    Resource:\n",
    "        https://arxiv.org/abs/2205.12647\n",
    "    \"\"\"\n",
    "    baseline_results = []\n",
    "    repeated_results = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        baseline_output = model.generate(**inputs)\n",
    "        repeated_output = model.generate(**inputs)\n",
    "\n",
    "        baseline_results.append(tokenizer.decode(baseline_output[0], skip_special_tokens=True))\n",
    "        repeated_results.append(tokenizer.decode(repeated_output[0], skip_special_tokens=True))\n",
    "\n",
    "    differences = [1 if b != r else 0 for b, r in zip(baseline_results, repeated_results)]\n",
    "    forgetting_rate = sum(differences) / len(prompts) * 100\n",
    "    return forgetting_rate\n",
    "\n",
    "def calculate_brevity_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate Brevity Score to evaluate concise text generation.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Brevity score.\n",
    "\n",
    "    Resource:\n",
    "        https://arxiv.org/pdf/1904.09675.pdf\n",
    "    \"\"\"\n",
    "    brevity_ratios = [len(pred.split()) / max(len(ref.split()), 1) for pred, ref in zip(predictions, references)]\n",
    "    brevity_score = np.mean([min(1.0, ratio) for ratio in brevity_ratios])\n",
    "    return {\"brevity_score\": brevity_score}\n",
    "\n",
    "\"\"\"\n",
    "Usage Example with Hugging Face LLM Mistral:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Sample inputs\n",
    "    predictions = [\"Climate change is a global challenge that requires...\"]\n",
    "    references = [[\"Climate change is a pressing issue affecting...\"]]\n",
    "\n",
    "    # BLEU\n",
    "    print(\"BLEU Score:\", calculate_bleu(predictions, references))\n",
    "\n",
    "    # ROUGE\n",
    "    print(\"ROUGE Score:\", calculate_rouge(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # METEOR\n",
    "    print(\"METEOR Score:\", calculate_meteor(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # BERTScore\n",
    "    print(\"BERTScore:\", calculate_bert_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # RAGAS\n",
    "    print(\"RAGAS Score:\", calculate_ragas_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # HELM\n",
    "    print(\"HELM Score:\", calculate_helm_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # GPT-Score\n",
    "    print(\"GPT-Score:\", calculate_gpt_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # Forgetting Rate\n",
    "    prompts = [\"What is climate change?\", \"Explain photosynthesis.\"]\n",
    "    print(\"Forgetting Rate:\", calculate_forgetting_rate(model, tokenizer, prompts))\n",
    "\n",
    "    # Brevity Score\n",
    "    print(\"Brevity Score:\", calculate_brevity_score(predictions, [r[0] for r in references]))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def calculate_perplexity(probabilities: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model's output.\n",
    "\n",
    "    Args:\n",
    "        probabilities (List[float]): A list of probabilities for each token in the sequence.\n",
    "\n",
    "    Returns:\n",
    "        float: The perplexity score.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/perplexity\n",
    "    \"\"\"\n",
    "    cross_entropy = -np.mean(np.log(probabilities))\n",
    "    perplexity = np.exp(cross_entropy)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision: float, recall: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the F1 score given precision and recall.\n",
    "\n",
    "    Args:\n",
    "        precision (float): Precision of the predictions.\n",
    "        recall (float): Recall of the predictions.\n",
    "\n",
    "    Returns:\n",
    "        float: The F1 score.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/f1\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def calculate_precision_recall(true_positive: int, false_positive: int, false_negative: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate precision and recall.\n",
    "\n",
    "    Args:\n",
    "        true_positive (int): Number of true positive cases.\n",
    "        false_positive (int): Number of false positive cases.\n",
    "        false_negative (int): Number of false negative cases.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing precision and recall scores.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/precision\n",
    "    \"\"\"\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "    return {\"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "def calculate_mean_reciprocal_rank(ranks: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Reciprocal Rank (MRR).\n",
    "\n",
    "    Args:\n",
    "        ranks (List[int]): A list of ranks for the first relevant result in each query.\n",
    "\n",
    "    Returns:\n",
    "        float: The MRR score.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/mrr\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = [1 / rank if rank > 0 else 0 for rank in ranks]\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "\n",
    "def calculate_mean_average_precision(relevance_scores: List[List[int]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision (MAP).\n",
    "\n",
    "    Args:\n",
    "        relevance_scores (List[List[int]]): A list of binary relevance scores for each query's retrieved documents.\n",
    "\n",
    "    Returns:\n",
    "        float: The MAP score.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/map\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    for scores in relevance_scores:\n",
    "        precision_at_k = [\n",
    "            sum(scores[:k + 1]) / (k + 1) for k in range(len(scores)) if scores[k] == 1\n",
    "        ]\n",
    "        if precision_at_k:\n",
    "            average_precisions.append(np.mean(precision_at_k))\n",
    "    return np.mean(average_precisions) if average_precisions else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce2d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b267b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3cfbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fca8f686",
   "metadata": {},
   "source": [
    "## Appendix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "metrics.py\n",
    "\n",
    "This module contains implementations and utilities to evaluate language models (LLMs) based on various\n",
    "metrics, including BLEU, ROUGE, METEOR, BERTScore, and others.\n",
    "\n",
    "The metrics are demonstrated with the Hugging Face model \"Mistral\" as an example.\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- torch\n",
    "- datasets\n",
    "- bert_score\n",
    "- nltk\n",
    "\n",
    "Install dependencies:\n",
    "    pip install transformers torch datasets bert-score nltk\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from bert_score import score\n",
    "\n",
    "# Example Hugging Face model for evaluation\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "def calculate_bleu(predictions: List[str], references: List[List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[List[str]]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: BLEU score and additional metrics.\n",
    "    \"\"\"\n",
    "    bleu_metric = load_metric(\"bleu\")\n",
    "    bleu_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = bleu_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_rouge(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: ROUGE scores.\n",
    "    \"\"\"\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    rouge_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = rouge_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_meteor(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate METEOR score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: METEOR score.\n",
    "    \"\"\"\n",
    "    meteor_metric = load_metric(\"meteor\")\n",
    "    meteor_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = meteor_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_bert_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate BERTScore for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Precision, Recall, and F1 scores.\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(predictions, references, lang=\"en\", verbose=True)\n",
    "    return {\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()}\n",
    "\n",
    "def calculate_ragas_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate RAGAS (Retrieval-Augmented Generation Answer Score).\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder RAGAS scores.\n",
    "    \"\"\"\n",
    "    # Implementation will depend on specific retrieval-augmented scoring methods\n",
    "    return {\"ragas_score\": 0.85}  # Placeholder value\n",
    "\n",
    "def calculate_helm_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate HELM (Holistic Evaluation of Language Models) metrics.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder HELM scores.\n",
    "    \"\"\"\n",
    "    # Implementation requires HELM evaluation framework\n",
    "    return {\"helm_score\": 0.9}  # Placeholder value\n",
    "\n",
    "def calculate_gpt_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate GPT-Score.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder GPT-Score.\n",
    "    \"\"\"\n",
    "    return {\"gpt_score\": 0.88}  # Placeholder value\n",
    "\n",
    "def scenario_fidelity_tests(predictions: List[str], scenarios: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function to evaluate Scenario Fidelity Tests.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        scenarios (List[str]): A list of test scenarios.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder scenario fidelity results.\n",
    "    \"\"\"\n",
    "    return {\"scenario_fidelity\": 0.92}  # Placeholder value\n",
    "\n",
    "def calculate_forgetting_rate(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, prompts: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate Forgetting Rate of the model over time.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): The language model to test.\n",
    "        tokenizer (AutoTokenizer): Tokenizer associated with the model.\n",
    "        prompts (List[str]): List of input prompts.\n",
    "\n",
    "    Returns:\n",
    "        float: Placeholder forgetting rate.\n",
    "    \"\"\"\n",
    "    return 0.05  # Placeholder value\n",
    "\n",
    "def rimu_evaluation(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function for RIMU (Relevance, Integration, Memory, Usefulness) evaluation.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder RIMU evaluation results.\n",
    "    \"\"\"\n",
    "    return {\"rimu_score\": 0.87}  # Placeholder value\n",
    "\n",
    "def problem_solving_effectiveness(predictions: List[str], problems: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate problem-solving effectiveness of the model.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted solutions from the model.\n",
    "        problems (List[str]): A list of problems to solve.\n",
    "\n",
    "    Returns:\n",
    "        float: Placeholder effectiveness score.\n",
    "    \"\"\"\n",
    "    return 0.91  # Placeholder value\n",
    "\n",
    "\"\"\"\n",
    "Usage Example with Hugging Face LLM Mistral:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Sample inputs\n",
    "    predictions = [\"Climate change is a global challenge that requires...\"]\n",
    "    references = [[\"Climate change is a pressing issue affecting...\"]]\n",
    "\n",
    "    # BLEU\n",
    "    print(\"BLEU Score:\", calculate_bleu(predictions, references))\n",
    "\n",
    "    # ROUGE\n",
    "    print(\"ROUGE Score:\", calculate_rouge(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # METEOR\n",
    "    print(\"METEOR Score:\", calculate_meteor(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # BERTScore\n",
    "    print(\"BERTScore:\", calculate_bert_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # RAGAS\n",
    "    print(\"RAGAS Score:\", calculate_ragas_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # HELM\n",
    "    print(\"HELM Score:\", calculate_helm_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # GPT-Score\n",
    "    print(\"GPT-Score:\", calculate_gpt_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # Scenario Fidelity\n",
    "    print(\"Scenario Fidelity:\", scenario_fidelity_tests(predictions, [\"scenario example\"]))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b95ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdae189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
