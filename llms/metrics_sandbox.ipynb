{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218173df",
   "metadata": {},
   "source": [
    "## Sandbox for LLM Metrics Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9525e03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128c3aa924b34ce58c2cdd13834d60d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### testing report generation with compute time tracking\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Any, List, Callable\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from pynvml import nvmlInit, nvmlDeviceGetCount, nvmlDeviceGetHandleByIndex, nvmlDeviceGetName\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "from utils import (\n",
    "    get_device,\n",
    "    get_hardware_info,\n",
    "    evaluate_latency_throughput,\n",
    "    evaluate_power_efficiency,\n",
    "    compare_precision_accuracy,\n",
    "    memory_by_sequence_length,\n",
    ")\n",
    "from metrics import (\n",
    "    calculate_perplexity,\n",
    "    calculate_f1_score,\n",
    "    calculate_precision_recall,\n",
    "    calculate_mean_reciprocal_rank,\n",
    "    calculate_mean_average_precision,\n",
    "    calculate_brevity_score,\n",
    "    #calculate_gpt_score,\n",
    "    #calculate_ragas_score,\n",
    "    #calculate_helm_score,\n",
    "    #calculate_forgetting_rate,\n",
    ")\n",
    "\n",
    "## Suppress annoying jupyter notebook specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Asking to truncate to max_length but no maximum length is provided\"\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Default to no truncation.\")\n",
    "# Suppress transformers library messages\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "## Use one of the below methods to login to huggingface\n",
    "## Option 1:\n",
    "#os.environ[\"HF_TOKEN\"] = \"your_access_token\" \n",
    "## Option 2:\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "\n",
    "# Timing decorator\n",
    "def timing_decorator(func: Callable) -> Callable:\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        wrapper.last_run_time = elapsed_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "\n",
    "    wrapper.last_run_time = 0\n",
    "    return wrapper\n",
    "\n",
    "# Evaluation functions with timing\n",
    "@timing_decorator\n",
    "def evaluate_latency_step(model, tokenizer, prompt, max_tokens, batch_size):\n",
    "    return evaluate_latency_throughput(model, tokenizer, prompt, max_tokens, batch_size)\n",
    "\n",
    "\n",
    "@timing_decorator\n",
    "def evaluate_power_step(model, tokenizer, prompt, max_tokens, batch_size):\n",
    "    return evaluate_power_efficiency(model, tokenizer, prompt, max_tokens, batch_size)\n",
    "\n",
    "\n",
    "@timing_decorator\n",
    "def evaluate_precision_step(model_name, prompt, max_tokens):\n",
    "    return compare_precision_accuracy(model_name, prompt, max_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97935aac",
   "metadata": {},
   "source": [
    "## Trying to get this stupid memory evaluation step working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc13e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def evaluate_memory(model, tokenizer, prompt, sequence_lengths, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Evaluate memory usage for varying sequence lengths, with device-agnostic behavior and proper memory measurement.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        prompt (str): The input prompt for the model.\n",
    "        sequence_lengths (List[int]): List of sequence lengths to evaluate.\n",
    "        max_new_tokens (int): The maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        dict: Memory usage for each sequence length.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nOptimized Memory Usage Evaluation (Device: {device.type.upper()}):\")\n",
    "\n",
    "    memory_results = {}\n",
    "\n",
    "    for length in sequence_lengths:\n",
    "        print(f\"  - Evaluating sequence length: {length}\")\n",
    "\n",
    "        # Truncate or pad the prompt to match the sequence length\n",
    "        input_text = prompt[:length]\n",
    "        encoded_input = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                if device.type == \"cuda\":\n",
    "                    # Clear GPU memory cache\n",
    "                    torch.cuda.empty_cache()\n",
    "                    memory_before = torch.cuda.memory_allocated(device)\n",
    "                    model.generate(\n",
    "                        input_ids=encoded_input[\"input_ids\"],\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                    )\n",
    "                    memory_after = torch.cuda.memory_allocated(device)\n",
    "                    memory_usage = (memory_after - memory_before) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "                else:\n",
    "                    # Measure memory on CPU\n",
    "                    process = psutil.Process()\n",
    "                    mem_before = process.memory_info().rss\n",
    "                    model.generate(\n",
    "                        input_ids=encoded_input[\"input_ids\"],\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                    )\n",
    "                    mem_after = process.memory_info().rss\n",
    "                    memory_usage = (mem_after - mem_before) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "                # Validate memory usage\n",
    "                if memory_usage <= 0:\n",
    "                    print(f\"    Warning: Memory usage for sequence length {length} was negligible.\")\n",
    "                memory_results[length] = memory_usage\n",
    "                print(f\"    Memory usage for sequence length {length}: {memory_usage:.2f} MB\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"    Failed to evaluate for sequence length {length}: {e}\")\n",
    "            memory_results[length] = \"OOM\"  # Out of Memory error\n",
    "\n",
    "    print(\"Memory evaluation completed.\\n\")\n",
    "    return memory_results\n",
    "\n",
    "\n",
    "\n",
    "# THE ONE BELOW WORKS BUT GIVES 0.0 MB for what's consumed in memory.\n",
    "# def evaluate_memory_step(model, tokenizer, prompt, sequence_lengths, max_new_tokens=1):\n",
    "#     \"\"\"\n",
    "#     Evaluate memory usage for varying sequence lengths, using `max_new_tokens` for token generation.\n",
    "\n",
    "#     Args:\n",
    "#         model: The model to evaluate.\n",
    "#         tokenizer: The tokenizer to use.\n",
    "#         prompt (str): The input prompt for the model.\n",
    "#         sequence_lengths (List[int]): List of sequence lengths to evaluate.\n",
    "#         max_new_tokens (int): The maximum number of tokens to generate.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Memory usage for each sequence length.\n",
    "#     \"\"\"\n",
    "#     print(\"\\nOptimized Memory Usage Evaluation:\")\n",
    "#     start_time = time.time()\n",
    "#     memory_results = {}\n",
    "\n",
    "#     for length in sequence_lengths:\n",
    "#         print(f\"  - Evaluating sequence length: {length}\")\n",
    "        \n",
    "#         # Truncate or pad the prompt to match the sequence length\n",
    "#         input_text = prompt[:length]\n",
    "#         encoded_input = tokenizer(\n",
    "#             input_text,\n",
    "#             return_tensors=\"pt\",\n",
    "#             max_length=length,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\"\n",
    "#         ).to(model.device)\n",
    "\n",
    "#         try:\n",
    "#             # Measure memory before and after generation\n",
    "#             with torch.no_grad():\n",
    "#                 memory_before = torch.cuda.memory_allocated(model.device)\n",
    "#                 model.generate(\n",
    "#                     input_ids=encoded_input[\"input_ids\"],\n",
    "#                     max_new_tokens=max_new_tokens,  # Fixed the issue here\n",
    "#                     pad_token_id=tokenizer.pad_token_id\n",
    "#                 )\n",
    "#                 memory_after = torch.cuda.memory_allocated(model.device)\n",
    "#                 memory_usage = (memory_after - memory_before) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "#             # Store and print results\n",
    "#             memory_results[length] = memory_usage\n",
    "#             print(f\"    Memory usage for sequence length {length}: {memory_usage:.2f} MB\")\n",
    "\n",
    "#         except RuntimeError as e:\n",
    "#             print(f\"    Failed to evaluate for sequence length {length}: {e}\")\n",
    "#             memory_results[length] = \"OOM\"  # Out of Memory error\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Memory evaluation completed in {elapsed_time:.2f} seconds.\\n\")\n",
    "#     return {\"memory_results\": memory_results, \"elapsed_time\": elapsed_time}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @timing_decorator\n",
    "# def evaluate_memory_step(model, tokenizer, prompt, sequence_lengths=[128], max_tokens=1):\n",
    "#     \"\"\"\n",
    "#     Evaluate memory usage for varying sequence lengths, optimized for performance and clear output.\n",
    "\n",
    "#     Args:\n",
    "#         model: The model to evaluate.\n",
    "#         tokenizer: The tokenizer to use.\n",
    "#         prompt (str): The input prompt for the model.\n",
    "#         sequence_lengths (List[int]): List of sequence lengths to evaluate.\n",
    "#         max_tokens (int): The maximum number of tokens to generate (default: 1).\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Memory usage for each sequence length.\n",
    "#     \"\"\"\n",
    "#     print(\"\\nOptimized Memory Usage Evaluation:\")\n",
    "#     memory_results = {}\n",
    "\n",
    "#     for length in sequence_lengths:\n",
    "#         print(f\"  - Evaluating sequence length: {length}\")\n",
    "        \n",
    "#         # Truncate or pad the prompt to match the sequence length\n",
    "#         input_text = prompt[:length]\n",
    "#         encoded_input = tokenizer(\n",
    "#             input_text,\n",
    "#             return_tensors=\"pt\",\n",
    "#             max_length=length,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\"\n",
    "#         ).to(model.device)\n",
    "\n",
    "#         try:\n",
    "#             # Measure memory before and after generation\n",
    "#             with torch.no_grad():\n",
    "#                 memory_before = torch.cuda.memory_allocated(model.device)\n",
    "#                 model.generate(input_ids=encoded_input[\"input_ids\"], max_length=max_tokens)\n",
    "#                 memory_after = torch.cuda.memory_allocated(model.device)\n",
    "#                 memory_usage = (memory_after - memory_before) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "#             # Store and print results\n",
    "#             memory_results[length] = memory_usage\n",
    "#             print(f\"    Memory usage for sequence length {length}: {memory_usage:.2f} MB\")\n",
    "\n",
    "#         except RuntimeError as e:\n",
    "#             print(f\"    Failed to evaluate for sequence length {length}: {e}\")\n",
    "#             memory_results[length] = \"OOM\"  # Out of Memory error\n",
    "\n",
    "#     print(\"Memory evaluation completed.\\n\")\n",
    "#     return memory_results\n",
    "\n",
    "\n",
    "# def evaluate_memory_step(model, tokenizer, prompt, max_tokens, sequence_lengths, verbose=True):\n",
    "#     memory_results = {}\n",
    "#     for length in sequence_lengths:\n",
    "#         if verbose:\n",
    "#             print(f\"  - Evaluating sequence length: {length}\")\n",
    "#         memory_usage = memory_by_sequence_length(model, tokenizer, prompt, max_tokens, length)\n",
    "#         memory_results[f\"sequence_length_{length}\"] = memory_usage\n",
    "#         if verbose:\n",
    "#             print(f\"    Memory usage for {length} tokens: {memory_usage} MB\")\n",
    "#     return memory_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f107f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_brevity(model, tokenizer, reference, prompt, max_new_tokens=50, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate brevity score for the generated text.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        reference (str): The reference text.\n",
    "        prompt (str): The input prompt for the model.\n",
    "        max_new_tokens (int): The maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        float: The brevity score (0 <= BP <= 1).\n",
    "    \"\"\"\n",
    "    # Encode the prompt\n",
    "    encoded_input = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Calculate lengths\n",
    "    reference_length = len(reference.split())\n",
    "    generated_length = len(generated_text.split())\n",
    "\n",
    "    # Calculate brevity penalty\n",
    "    if generated_length == 0:\n",
    "        print(\"Warning: Generated text is empty. Returning BP = 0.\")\n",
    "        return 0.0\n",
    "    elif generated_length >= reference_length:\n",
    "        brevity_penalty = 1.0  # No penalty for long outputs\n",
    "    else:\n",
    "        brevity_penalty = math.exp(1 - (reference_length / generated_length))  # Short output penalty\n",
    "    \n",
    "    if verbose:\n",
    "        return brevity_penalty, generated_text\n",
    "    else:\n",
    "        return brevity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dade2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8353510d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a7c2fef6634cabb484a656b16d6099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f75abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7157418a6c4f87b21319d65c72696e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils import get_device\n",
    "\n",
    "device = \"cpu\" #get_device()\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08364bb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '</s>' or '<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0663d387",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prompt = \"Write the continuation of this text in fluent English:\\n\\n\" + \"It was the best of times, it was the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d83ebd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.6 s, sys: 2.39 s, total: 60 s\n",
      "Wall time: 56.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write the continuation of this text in fluent English:\\n\\nIt was the best of times, it was the worst of times. It was the age of wisdom, it was the age of foolishness. It was the epoch of belief, it was the epoch of incredulity. It was the season of Light, it was the season of Darkness,'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "encoded_input = tokenizer(prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=encoded_input[\"input_ids\"],\n",
    "    max_new_tokens=50,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = (\n",
    "    \"It was the best of times, it was the worst of times, it was the age of wisdom, \"\n",
    "    \"it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, \"\n",
    "    \"it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\n",
    ")\n",
    "\n",
    "# Evaluate brevity score\n",
    "brevity_score, gen_text = evaluate_brevity(model, tokenizer, reference_text, prompt, verbose=True)\n",
    "print(f\"Brevity Score: {brevity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26592f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    \"It was the best of times, it was the worst of times, it was the age of wisdom, \"\n",
    "    \"it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, \"\n",
    "    \"it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8233c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f1dfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfaf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ccfe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2063cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Correct way to load the model\n",
    "# # model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "# # model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# # model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token or '<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfe8f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brevity Score: 0.2231\n"
     ]
    }
   ],
   "source": [
    "# Reference and prompt\n",
    "reference_text = (\n",
    "    \"It was the best of times, it was the worst of times, it was the age of wisdom, \"\n",
    "    \"it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, \"\n",
    "    \"it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\n",
    ")\n",
    "prompt = \"It was the best of times, it was the worst of\"\n",
    "\n",
    "# Evaluate brevity score\n",
    "brevity_score, gen_text = evaluate_brevity(model, tokenizer, reference_text, prompt, verbose=True)\n",
    "print(f\"Brevity Score: {brevity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfc1399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write the continuation of this text in fluent English:\\n\\n\" + \"It was the best of times, it was the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91c8ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ce3d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 8s, sys: 1min 33s, total: 14min 42s\n",
      "Wall time: 1min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write the continuation of this text in fluent English:\\n\\nIt was the best of times, it was the most of the. |\\n\\n 1908 the 1880—1890 гг.\\n\\n1895 году, по указу стало 12000000.\\n\\n1900 году, по указу стало 12000000.\\n\\n1905 году, по указу стало 120000000.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "outputs = model.generate(\n",
    "    input_ids=encoded_input[\"input_ids\"],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    temperature=0.7,  # Adjust creativity\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18811f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bfb0974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write the continuation of this text in fluent English:\\n\\nThe quick brown fox jumped over the fence, the first time.\\n\\n.\\n\\n.\\n.\\n\\n.\\n.\\n.\\n\\n и о.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11112801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "624692a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What did the quick brown fox jump over?\\n\\nчиследование нашей, 20107 |\\n| 15 18 17 19 20 25 29 30 37 42 4'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc7ab1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Complete this quote, 'It was the best of times, it was the worst of'\\n\\nи в 1858 году, когда вскоре после начала правления на царство уже входил в план единственной епархии, — в 1837 году была\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817c4213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was the best of times, it was the worst of times, it was the age of wisdom,  and also,  when they are not, it is also possible to 1852, и в настоящем году на 21-ым. 1852 года на 1852 году.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e0e3e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was the best of times, it was the worst of       and 1980-х годах.\\n\\n 1981 году.\\n\\n## 1982 году.\\n\\n## 1983 году.\\n\\n## 1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663685a",
   "metadata": {},
   "source": [
    "### Next ones to make work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba150f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def evaluate_metrics_step(probabilities, precision, recall, true_positive, false_positive, false_negative, ranks, relevance_scores, prompt, outputs):\n",
    "    return {\n",
    "        \"perplexity\": calculate_perplexity(probabilities),\n",
    "        \"f1_score\": calculate_f1_score(precision, recall),\n",
    "        \"precision_recall\": calculate_precision_recall(true_positive, false_positive, false_negative),\n",
    "        \"mrr\": calculate_mean_reciprocal_rank(ranks),\n",
    "        \"map\": calculate_mean_average_precision(relevance_scores),\n",
    "        \"brevity_score\": calculate_brevity_score(prompt, outputs),\n",
    "        #\"gpt_score\": calculate_gpt_score(prompt, outputs),\n",
    "#         \"ragas_score\": calculate_ragas_score(prompt, outputs),\n",
    "#         \"helm_score\": calculate_helm_score(prompt, outputs),\n",
    "#         \"forgetting_rate\": calculate_forgetting_rate(prompt, outputs),\n",
    "    }\n",
    "\n",
    "# Trying to estimate runtime\n",
    "def get_estimated_runtime(selected_steps: List[str], average_times: Dict[str, float]) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the total runtime based on selected steps.\n",
    "\n",
    "    Args:\n",
    "        selected_steps (List[str]): List of evaluation steps to perform.\n",
    "        average_times (Dict[str, float]): Average times for each step.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated runtime in seconds.\n",
    "    \"\"\"\n",
    "    return sum(average_times.get(step, 0) for step in selected_steps)\n",
    "\n",
    "def evaluate_model(\n",
    "    model_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    evaluate_latency: bool = True,\n",
    "    evaluate_power: bool = True,\n",
    "    evaluate_precision: bool = True,\n",
    "    evaluate_memory: bool = True,\n",
    "    evaluate_metrics: bool = True,\n",
    "    quantized: bool = False,\n",
    "    prompt: str = \"What is the impact of climate change?\",\n",
    "    batch_size: int = 4,\n",
    "    max_tokens: int = 128,\n",
    "    sequence_lengths: List[int] = [128],#, 512], #256, 512, 1024],\n",
    "    relevance_scores: List[List[int]] = [[1, 0, 1, 1, 0]],\n",
    "    probabilities: List[float] = [0.2, 0.3, 0.1, 0.4],\n",
    "    ranks: List[int] = [1, 3, 2, 0],\n",
    "    precision: float = 0.8,\n",
    "    recall: float = 0.75,\n",
    "    true_positive: int = 50,\n",
    "    false_positive: int = 10,\n",
    "    false_negative: int = 15,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate an LLM using specified metrics and utilities.\n",
    "    \"\"\"\n",
    "    report = {\"model_name\": model_name, \"evaluation_results\": {}, \"conditions\": {}, \"timing\": {}}\n",
    "\n",
    "    # Detect device\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '</s>' or '<|endoftext|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Hardware and conditions\n",
    "    model_parameters = sum(p.numel() for p in model.parameters())\n",
    "    report[\"conditions\"] = {\n",
    "        \"prompt\": prompt,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"sequence_lengths\": sequence_lengths,\n",
    "        \"device\": device.type,\n",
    "        \"model_parameters\": model_parameters,\n",
    "        \"quantized\": quantized,\n",
    "    }\n",
    "\n",
    "    # Step execution\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    if evaluate_latency:\n",
    "        print(\"Evaluating latency...\")\n",
    "        outputs, latency, throughput, token_throughput = evaluate_latency_step(model, tokenizer, prompt, max_tokens, batch_size)\n",
    "        report[\"evaluation_results\"][\"latency_throughput\"] = {\n",
    "            \"latency\": latency,\n",
    "            \"throughput\": throughput,\n",
    "            \"token_throughput\": token_throughput,\n",
    "        }\n",
    "        report[\"timing\"][\"latency\"] = evaluate_latency_step.last_run_time\n",
    "\n",
    "    if evaluate_power and device.type == \"cuda\":\n",
    "        print(\"Evaluating power efficiency...\")\n",
    "        power_consumed, energy_per_token = evaluate_power_step(model, tokenizer, prompt, max_tokens, batch_size)\n",
    "        report[\"evaluation_results\"][\"power_efficiency\"] = {\n",
    "            \"power_consumed\": power_consumed,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "        }\n",
    "        report[\"timing\"][\"power_efficiency\"] = evaluate_power_step.last_run_time\n",
    "\n",
    "    if evaluate_precision:\n",
    "        print(\"Comparing precision...\")\n",
    "        precision_match = evaluate_precision_step(model_name, prompt, max_tokens)\n",
    "        report[\"evaluation_results\"][\"precision_comparison\"] = {\"precision_match\": precision_match}\n",
    "        report[\"timing\"][\"precision_comparison\"] = evaluate_precision_step.last_run_time\n",
    "\n",
    "    if evaluate_memory:\n",
    "        print(\"Evaluating memory usage...\")\n",
    "        memory_eval = evaluate_memory_step(model, tokenizer, prompt, sequence_lengths, max_tokens)\n",
    "        report[\"evaluation_results\"][\"memory_usage\"] = memory_eval[\"memory_results\"]\n",
    "        report[\"timing\"][\"memory_evaluation\"] = memory_eval[\"elapsed_time\"]\n",
    "\n",
    "    if evaluate_metrics:\n",
    "        print(\"Evaluating metrics...\")\n",
    "        metrics_results = evaluate_metrics_step(\n",
    "            probabilities, precision, recall, true_positive, false_positive, false_negative, ranks, relevance_scores, prompt, outputs\n",
    "        )\n",
    "        report[\"evaluation_results\"][\"metrics\"] = metrics_results\n",
    "        report[\"timing\"][\"metrics\"] = evaluate_metrics_step.last_run_time\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    report[\"timing\"][\"total_runtime\"] = total_end_time - total_start_time\n",
    "\n",
    "    print(f\"Total runtime: {report['timing']['total_runtime']:.2f} seconds\")\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3e7f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated runtime: 6.00 seconds\n",
      "Using device: cpu\n",
      "Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcfb2259e494bbab839ef80c151956d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating latency...\n",
      "Warming up...\n",
      "Warm-up Time: 173.59s\n",
      "Measuring latency and throughput...\n"
     ]
    }
   ],
   "source": [
    "average_times = {\n",
    "    \"evaluate_latency\": 2.0,\n",
    "    \"evaluate_power\": 1.5,\n",
    "    \"evaluate_precision\": 1.8,\n",
    "    \"evaluate_memory\": 2.2,\n",
    "    \"evaluate_metrics\": 2.5,\n",
    "}\n",
    "\n",
    "selected_steps = [\"evaluate_latency\", \"evaluate_power\", \"evaluate_metrics\"]\n",
    "estimated_runtime = get_estimated_runtime(selected_steps, average_times)\n",
    "print(f\"Estimated runtime: {estimated_runtime:.2f} seconds\")\n",
    "\n",
    "##TODO: Setup pipelines for use with gpt2-xl, see https://huggingface.co/openai-community/gpt2-xl\n",
    "evaluation_report = evaluate_model(\n",
    "#     model_name=\"mistralai/Mistral-7B-v0.1\",\n",
    "#     model_name=\"mistralai/Mistral-7B-v0.2\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#     model_name = \"deepseek-ai/DeepSeek-V2.5\",\n",
    "#     model_name = \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    evaluate_latency=True,\n",
    "    evaluate_power=True,\n",
    "    evaluate_precision=True,\n",
    "    evaluate_memory=True,\n",
    "    evaluate_metrics=False,\n",
    "    quantized=False,\n",
    ")\n",
    "print(\"\\nEvaluation Report:\")\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4b97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b303e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea45310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47b05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088a18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ccaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53df5bfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated runtime: 6.00 seconds\n",
      "Using device: mps\n",
      "Loading model: mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70a24cf6bf144c0a41635df7a4012fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating latency...\n",
      "Warming up...\n",
      "Warm-up Time: 9.59s\n",
      "Measuring latency and throughput...\n",
      "Latency: 7.73s | Throughput: 0.52 responses/sec | Token Throughput: 25.89 tokens/sec\n",
      "evaluate_latency_step took 17.33 seconds\n",
      "Comparing precision...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5739154a383f4dd3b321420b640f936e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c9b079eefe453aa3fe365734aed14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision (fp32) Output:\n",
      "What is the impact of climate change?\n",
      "\n",
      "Climate change is a global phenomenon that is affecting the entire planet. It is caused by the increase in greenhouse gases in the atmosphere, which trap heat and cause the Earth’s temperature to rise. This has led to a number\n",
      "Mixed Precision (fp16) Output:\n",
      "What is the impact of climate change?\n",
      "\n",
      "Climate change is a global phenomenon that is affecting the entire planet. It is caused by the increase in greenhouse gases in the atmosphere, which trap heat and cause the Earth’s temperature to rise. This has led to a number\n",
      "evaluate_precision_step took 103.05 seconds\n",
      "Evaluating memory usage...\n",
      "  - Evaluating sequence length: 128\n",
      "    Memory usage for 128 tokens: {128: 0.0, 256: 0.0, 512: 0.0} MB\n",
      "  - Evaluating sequence length: 512\n",
      "    Memory usage for 512 tokens: {128: 0.0, 256: 0.0, 512: 0.0} MB\n",
      "evaluate_memory_step took 164.52 seconds\n",
      "Evaluating metrics...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "split() missing 1 required positional argument: 'split_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m estimated_runtime \u001b[38;5;241m=\u001b[39m get_estimated_runtime(selected_steps, average_times)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated runtime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_runtime\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m evaluation_report \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-v0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_latency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluation_report)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model_name, evaluate_latency, evaluate_power, evaluate_precision, evaluate_memory, evaluate_metrics, quantized, prompt, batch_size, max_tokens, sequence_lengths, relevance_scores, probabilities, ranks, precision, recall, true_positive, false_positive, false_negative)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_metrics:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating metrics...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m     metrics_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_metrics_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_positive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfalse_positive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfalse_negative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevance_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_results\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metrics_results\n\u001b[1;32m    163\u001b[0m     report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiming\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_metrics_step\u001b[38;5;241m.\u001b[39mlast_run_time\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtiming_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      4\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mevaluate_metrics_step\u001b[0;34m(probabilities, precision, recall, true_positive, false_positive, false_negative, ranks, relevance_scores, prompt, outputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@timing_decorator\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_metrics_step\u001b[39m(probabilities, precision, recall, true_positive, false_positive, false_negative, ranks, relevance_scores, prompt, outputs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_perplexity(probabilities),\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_f1_score(precision, recall),\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_precision_recall(true_positive, false_positive, false_negative),\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrr\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_mean_reciprocal_rank(ranks),\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_mean_average_precision(relevance_scores),\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m#\"gpt_score\": calculate_gpt_score(prompt, outputs),\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrevity_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mcalculate_brevity_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mragas_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_ragas_score(prompt, outputs),\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelm_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_helm_score(prompt, outputs),\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforgetting_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_forgetting_rate(prompt, outputs),\n\u001b[1;32m     57\u001b[0m     }\n",
      "File \u001b[0;32m~/Desktop/utils/llms/metrics.py:206\u001b[0m, in \u001b[0;36mcalculate_brevity_score\u001b[0;34m(predictions, references)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_brevity_score\u001b[39m(predictions: List[\u001b[38;5;28mstr\u001b[39m], references: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    Calculate Brevity Score to evaluate concise text generation.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m        https://arxiv.org/pdf/1904.09675.pdf\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     brevity_ratios \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(pred\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ref\u001b[38;5;241m.\u001b[39msplit()), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pred, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, references)]\n\u001b[1;32m    207\u001b[0m     brevity_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, ratio) \u001b[38;5;28;01mfor\u001b[39;00m ratio \u001b[38;5;129;01min\u001b[39;00m brevity_ratios])\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrevity_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: brevity_score}\n",
      "File \u001b[0;32m~/Desktop/utils/llms/metrics.py:206\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_brevity_score\u001b[39m(predictions: List[\u001b[38;5;28mstr\u001b[39m], references: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    Calculate Brevity Score to evaluate concise text generation.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m        https://arxiv.org/pdf/1904.09675.pdf\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     brevity_ratios \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(pred\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pred, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, references)]\n\u001b[1;32m    207\u001b[0m     brevity_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, ratio) \u001b[38;5;28;01mfor\u001b[39;00m ratio \u001b[38;5;129;01min\u001b[39;00m brevity_ratios])\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrevity_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: brevity_score}\n",
      "\u001b[0;31mTypeError\u001b[0m: split() missing 1 required positional argument: 'split_size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd91f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df4a63a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated runtime: 6.00 seconds\n",
      "Using device: mps\n",
      "Loading model: mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating latency...\n",
      "Warming up...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up Time: 9.61s\n",
      "Measuring latency and throughput...\n",
      "Latency: 7.70s | Throughput: 0.52 responses/sec | Token Throughput: 25.97 tokens/sec\n",
      "evaluate_latency_step took 17.32 seconds\n",
      "Comparing precision...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision (fp32) Output:\n",
      "What is the impact of climate change?\n",
      "\n",
      "Climate change is a global phenomenon that is affecting the entire planet. It is caused by the increase in greenhouse gases in the atmosphere, which trap heat and cause the Earth’s temperature to rise. This has led to a number\n",
      "Mixed Precision (fp16) Output:\n",
      "What is the impact of climate change?\n",
      "\n",
      "Climate change is a global phenomenon that is affecting the entire planet. It is caused by the increase in greenhouse gases in the atmosphere, which trap heat and cause the Earth’s temperature to rise. This has led to a number\n",
      "evaluate_precision_step took 115.20 seconds\n",
      "Evaluating memory usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_memory_step took 241.61 seconds\n",
      "Evaluating metrics...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m estimated_runtime \u001b[38;5;241m=\u001b[39m get_estimated_runtime(selected_steps, average_times)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated runtime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_runtime\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m evaluation_report \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-v0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_latency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluation_report)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model_name, evaluate_latency, evaluate_power, evaluate_precision, evaluate_memory, evaluate_metrics, quantized, prompt, batch_size, max_tokens, sequence_lengths, relevance_scores, probabilities, ranks, precision, recall, true_positive, false_positive, false_negative)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_metrics:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating metrics...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m     metrics_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_metrics_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_positive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfalse_positive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfalse_negative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevance_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_results\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metrics_results\n\u001b[1;32m    159\u001b[0m     report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiming\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_metrics_step\u001b[38;5;241m.\u001b[39mlast_run_time\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtiming_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      4\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mevaluate_metrics_step\u001b[0;34m(probabilities, precision, recall, true_positive, false_positive, false_negative, ranks, relevance_scores, prompt, outputs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@timing_decorator\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_metrics_step\u001b[39m(probabilities, precision, recall, true_positive, false_positive, false_negative, ranks, relevance_scores, prompt, outputs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_perplexity(probabilities),\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_f1_score(precision, recall),\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_precision_recall(true_positive, false_positive, false_negative),\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrr\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_mean_reciprocal_rank(ranks),\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_mean_average_precision(relevance_scores),\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mcalculate_gpt_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mragas_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_ragas_score(prompt, outputs),\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelm_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_helm_score(prompt, outputs),\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforgetting_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_forgetting_rate(prompt, outputs),\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrevity_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_brevity_score(prompt, outputs),\n\u001b[1;32m     53\u001b[0m     }\n",
      "File \u001b[0;32m~/Desktop/utils/llms/metrics.py:157\u001b[0m, in \u001b[0;36mcalculate_gpt_score\u001b[0;34m(predictions, references)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_gpt_score\u001b[39m(predictions: List[\u001b[38;5;28mstr\u001b[39m], references: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Calculate GPT-Score for text similarity.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m        https://github.com/IntelLabs/gpt-score\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTScorer\n\u001b[1;32m    158\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m GPTScorer()\n\u001b[1;32m    159\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scorer\u001b[38;5;241m.\u001b[39mscore(predictions, references)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_score'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c7d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23898281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7fccff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669dddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c5c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34ee40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dbae3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb857f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4225f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf767c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf183a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model_name: str = \"mistralai/Mistral-7B-v0.1\",\n",
    "    evaluate_latency: bool = True,\n",
    "    evaluate_power: bool = True,\n",
    "    evaluate_precision: bool = True,\n",
    "    evaluate_memory: bool = True,\n",
    "    evaluate_metrics: bool = True,\n",
    "    quantized: bool = False,\n",
    "    prompt: str = \"What is the impact of climate change?\",\n",
    "    batch_size: int = 4,\n",
    "    max_tokens: int = 50,\n",
    "    sequence_lengths: List[int] = [128, 256, 512, 1024],\n",
    "    relevance_scores: List[List[int]] = [[1, 0, 1, 1, 0]],\n",
    "    probabilities: List[float] = [0.2, 0.3, 0.1, 0.4],\n",
    "    ranks: List[int] = [1, 3, 2, 0],\n",
    "    precision: float = 0.8,\n",
    "    recall: float = 0.75,\n",
    "    true_positive: int = 50,\n",
    "    false_positive: int = 10,\n",
    "    false_negative: int = 15,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate an LLM using specified metrics and utilities.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The Hugging Face model name or path.\n",
    "        evaluate_latency (bool): Whether to evaluate latency and throughput.\n",
    "        evaluate_power (bool): Whether to evaluate power efficiency.\n",
    "        evaluate_precision (bool): Whether to compare precision between fp32 and fp16.\n",
    "        evaluate_memory (bool): Whether to measure memory usage for varying sequence lengths.\n",
    "        evaluate_metrics (bool): Whether to compute various metrics like perplexity, F1 score, etc.\n",
    "        quantized (bool): Whether the model is quantized or not.\n",
    "        prompt (str): The input prompt for the model.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        max_tokens (int): Maximum tokens to generate.\n",
    "        sequence_lengths (List[int]): List of sequence lengths for memory evaluation.\n",
    "        relevance_scores (List[List[int]]): Relevance scores for MAP calculation.\n",
    "        probabilities (List[float]): Probabilities for perplexity calculation.\n",
    "        ranks (List[int]): Ranks for MRR calculation.\n",
    "        precision (float): Precision score for F1 calculation.\n",
    "        recall (float): Recall score for F1 calculation.\n",
    "        true_positive (int): True positives for precision/recall calculation.\n",
    "        false_positive (int): False positives for precision/recall calculation.\n",
    "        false_negative (int): False negatives for precision/recall calculation.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A detailed report of all evaluations performed.\n",
    "\n",
    "    Example:\n",
    "        report = evaluate_model(\"meta-llama/Llama-2-7b-hf\", evaluate_latency=True, evaluate_metrics=True)\n",
    "        print(report)\n",
    "    \"\"\"\n",
    "    report = {\"model_name\": model_name, \"evaluation_results\": {}, \"conditions\": {}}\n",
    "\n",
    "    # Detect device\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token or '<|endoftext|>'})\n",
    "        print(f\"Assigned {tokenizer.pad_token} as the `pad_token`.\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True).to(device)\n",
    "\n",
    "    # Hardware information\n",
    "    hardware_info = get_hardware_info()\n",
    "    model_parameters = sum(p.numel() for p in model.parameters())\n",
    "    report[\"conditions\"] = {\n",
    "        \"prompt\": prompt,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"sequence_lengths\": sequence_lengths,\n",
    "        \"num_gpus\": hardware_info[\"num_gpus\"],\n",
    "        \"gpu_types\": hardware_info[\"gpu_types\"],\n",
    "        \"device\": hardware_info[\"device\"],\n",
    "        \"model_parameters\": model_parameters,\n",
    "        \"quantized\": quantized,\n",
    "    }\n",
    "\n",
    "    # Latency and throughput evaluation\n",
    "    if evaluate_latency:\n",
    "        print(\"Evaluating latency and throughput...\")\n",
    "        outputs, latency, throughput, token_throughput = evaluate_latency_throughput(\n",
    "            model, tokenizer, prompt, max_tokens, batch_size\n",
    "        )\n",
    "        report[\"evaluation_results\"][\"latency_throughput\"] = {\n",
    "            \"latency\": latency,\n",
    "            \"throughput\": throughput,\n",
    "            \"token_throughput\": token_throughput,\n",
    "        }\n",
    "\n",
    "    # Power efficiency evaluation\n",
    "    if evaluate_power and device.type == \"cuda\":\n",
    "        print(\"Evaluating power efficiency...\")\n",
    "        power_consumed, energy_per_token = evaluate_power_efficiency(\n",
    "            model, tokenizer, prompt, max_tokens, batch_size\n",
    "        )\n",
    "        report[\"evaluation_results\"][\"power_efficiency\"] = {\n",
    "            \"power_consumed\": power_consumed,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "        }\n",
    "    elif evaluate_power:\n",
    "        print(\"Power efficiency evaluation is only supported on CUDA devices.\")\n",
    "\n",
    "    # Precision comparison\n",
    "    if evaluate_precision:\n",
    "        print(\"Comparing precision...\")\n",
    "        precision_match = compare_precision_accuracy(model_name, prompt, max_tokens)\n",
    "        report[\"evaluation_results\"][\"precision_comparison\"] = {\"precision_match\": precision_match}\n",
    "\n",
    "    # Memory evaluation\n",
    "    if evaluate_memory:\n",
    "        print(\"Evaluating memory usage...\")\n",
    "        memory_results = {}\n",
    "        for length in sequence_lengths:\n",
    "            memory_usage = memory_by_sequence_length(model, tokenizer, prompt, max_tokens, length)\n",
    "            memory_results[f\"sequence_length_{length}\"] = memory_usage\n",
    "        report[\"evaluation_results\"][\"memory_usage\"] = memory_results\n",
    "\n",
    "    # Metrics evaluation\n",
    "    if evaluate_metrics:\n",
    "        print(\"Evaluating metrics...\")\n",
    "        metrics_results = {\n",
    "            \"perplexity\": calculate_perplexity(probabilities),\n",
    "            \"f1_score\": calculate_f1_score(precision, recall),\n",
    "            \"precision_recall\": calculate_precision_recall(true_positive, false_positive, false_negative),\n",
    "            \"mrr\": calculate_mean_reciprocal_rank(ranks),\n",
    "            \"map\": calculate_mean_average_precision(relevance_scores),\n",
    "            \"gpt_score\": calculate_gpt_score(prompt, outputs),\n",
    "            \"ragas_score\": calculate_ragas_score(prompt, outputs),\n",
    "            \"helm_score\": calculate_helm_score(prompt, outputs),\n",
    "            \"forgetting_rate\": calculate_forgetting_rate(prompt, outputs),\n",
    "            \"brevity_score\": calculate_brevity_score(prompt, outputs),\n",
    "        }\n",
    "        report[\"evaluation_results\"][\"metrics\"] = metrics_results\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab336053",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading model: mistralai/Mistral-7B-v0.1\n",
      "Assigned </s> as the `pad_token`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathan/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/nathan/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fa75bced524ece9691c306ce9ad93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating latency and throughput...\n",
      "Warming up...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up Time: 12.97s\n",
      "Measuring latency and throughput...\n",
      "Latency: 7.74s | Throughput: 0.52 responses/sec | Token Throughput: 25.85 tokens/sec\n",
      "Power efficiency evaluation is only supported on CUDA devices.\n",
      "Comparing precision...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44f32b0e94341a8b6f9ebb6998651d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96eda3c7ce949a78c29f5f2c7c83213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision (fp32) Output:\n",
      "What is the impact of climate change?\n",
      "\n",
      "Climate change is a global phenomenon that is affecting the entire planet. It is caused by the increase in greenhouse gases in the atmosphere, which trap heat and cause the Earth’s temperature to rise. This has led to a number\n",
      "Mixed Precision (fp16) Output:\n",
      "What is the impact of climate change?\n",
      "\n",
      "Climate change is a global phenomenon that is affecting the entire planet. It is caused by the increase in greenhouse gases in the atmosphere, which trap heat and cause the Earth’s temperature to rise. This has led to a number\n",
      "Evaluating memory usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metrics...\n",
      "\n",
      "Evaluation Report:\n",
      "{'model_name': 'mistralai/Mistral-7B-v0.1', 'evaluation_results': {'latency_throughput': {'latency': 7.737421035766602, 'throughput': 0.5169681191588007, 'token_throughput': 25.848405957940038}, 'precision_comparison': {'precision_match': True}, 'memory_usage': {'sequence_length_128': {128: 0.0, 256: 0.0, 512: 0.0}, 'sequence_length_256': {128: 0.0, 256: 0.0, 512: 0.0}, 'sequence_length_512': {128: 0.0, 256: 0.0, 512: 0.0}, 'sequence_length_1024': {128: 0.0, 256: 0.0, 512: 0.0, 1024: 0.0}}, 'metrics': {'perplexity': 4.518010018049225, 'f1_score': 0.7741935483870969, 'precision_recall': {'precision': 0.8333333333333334, 'recall': 0.7692307692307693}, 'mrr': 0.4583333333333333, 'map': 0.8055555555555555}}, 'conditions': {'prompt': 'What is the impact of climate change?', 'batch_size': 4, 'max_tokens': 50, 'sequence_lengths': [128, 256, 512, 1024], 'num_gpus': 1, 'gpu_types': ['Apple M1/M2/M3 (Metal Performance Shaders)'], 'device': 'mps', 'model_parameters': 7241732096, 'quantized': False}}\n"
     ]
    }
   ],
   "source": [
    "evaluation_report = evaluate_model(\n",
    "    model_name=\"mistralai/Mistral-7B-v0.1\",\n",
    "#     model_name=\"mistralai/Mistral-7B-v0.2\",\n",
    "#     model_name=\"mistralai/Mistral-7B-v0.3\",\n",
    "    evaluate_latency=True,\n",
    "    evaluate_power=True,\n",
    "    evaluate_precision=True,\n",
    "    evaluate_memory=True,\n",
    "    evaluate_metrics=True,\n",
    "    quantized=False,\n",
    ")\n",
    "print(\"\\nEvaluation Report:\")\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37482ec7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d0a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1699b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e6f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86c959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # Ensure this import exists\n",
    "\n",
    "def compare_precision_accuracy(\n",
    "    model_name: str, prompt: str, max_tokens: int = 50\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Compare outputs for fp32 and fp16 precision to test output integrity.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The Hugging Face model name or path.\n",
    "        prompt (str): The input prompt for the model.\n",
    "        max_tokens (int): The maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the outputs for fp32 and fp16 are identical.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Full precision (fp32)\n",
    "    model_fp32 = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output_fp32 = model_fp32.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    text_fp32 = tokenizer.decode(output_fp32[0], skip_special_tokens=True)\n",
    "\n",
    "    # Mixed precision (fp16)\n",
    "    model_fp16 = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    output_fp16 = model_fp16.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    text_fp16 = tokenizer.decode(output_fp16[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Full Precision (fp32) Output:\\n{text_fp32}\")\n",
    "    print(f\"Mixed Precision (fp16) Output:\\n{text_fp16}\")\n",
    "\n",
    "    return text_fp32 == text_fp16\n",
    "Checklist of Fixes\n",
    "Add the Missing Import: Ensure that the following is present at the top of the script:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "Verify get_device Function: Confirm that the get_device() function is defined to return the appropriate device (cuda, mps, or cpu).\n",
    "\n",
    "Test the Function: Use the following test code to verify that the compare_precision_accuracy function works as intended:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "    prompt = \"What is the impact of climate change?\"\n",
    "    precision_match = compare_precision_accuracy(model_name, prompt)\n",
    "    print(f\"Precision Match: {precision_match}\")\n",
    "Expected Output\n",
    "For a properly implemented compare_precision_accuracy function, you should see something like:\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "Using device: cuda\n",
    "Full Precision (fp32) Output:\n",
    "The impact of climate change includes rising temperatures, sea-level rise...\n",
    "Mixed Precision (fp16) Output:\n",
    "The impact of climate change includes rising temperatures, sea-level rise...\n",
    "Precision Match: True\n",
    "If the outputs for fp32 and fp16 differ, the function will return False.\n",
    "\n",
    "Let me know if you encounter any additional issues!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ChatGPT can make mistakes. Check important info.\n",
    "?\n",
    "\n",
    "Metrics Utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Run\n",
    "\n",
    "\n",
    "133\n",
    "134\n",
    "135\n",
    "136\n",
    "137\n",
    "138\n",
    "139\n",
    "140\n",
    "141\n",
    "142\n",
    "143\n",
    "144\n",
    "145\n",
    "146\n",
    "147\n",
    "148\n",
    "149\n",
    "150\n",
    "151\n",
    "152\n",
    "153\n",
    "154\n",
    "155\n",
    "156\n",
    "157\n",
    "158\n",
    "159\n",
    "160\n",
    "161\n",
    "162\n",
    "163\n",
    "164\n",
    "165\n",
    "166\n",
    "167\n",
    "168\n",
    "169\n",
    "170\n",
    "171\n",
    "172\n",
    "173\n",
    "174\n",
    "175\n",
    "176\n",
    "177\n",
    "178\n",
    "179\n",
    "180\n",
    "181\n",
    "182\n",
    "183\n",
    "184\n",
    "185\n",
    "186\n",
    "187\n",
    "188\n",
    "189\n",
    "190\n",
    "191\n",
    "192\n",
    "193\n",
    "194\n",
    "195\n",
    "196\n",
    "197\n",
    "198\n",
    "199\n",
    "200\n",
    "201\n",
    "202\n",
    "203\n",
    "204\n",
    "205\n",
    "206\n",
    "207\n",
    "208\n",
    "209\n",
    "210\n",
    "211\n",
    "212\n",
    "213\n",
    "214\n",
    "215\n",
    "216\n",
    "217\n",
    "218\n",
    "219\n",
    "220\n",
    "221\n",
    "222\n",
    "223\n",
    "224\n",
    "225\n",
    "226\n",
    "227\n",
    "228\n",
    "229\n",
    "230\n",
    "231\n",
    "232\n",
    "233\n",
    "234\n",
    "235\n",
    "236\n",
    "237\n",
    "238\n",
    "239\n",
    "240\n",
    "241\n",
    "242\n",
    "243\n",
    "244\n",
    "245\n",
    "246\n",
    "247\n",
    "248\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8f770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b902aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "utils.py\n",
    "\n",
    "This module contains utilities for evaluating the performance of large language models (LLMs) using\n",
    "metrics such as latency, throughput, power consumption, memory usage, and precision comparison.\n",
    "It is designed for use with Hugging Face's open-source LLMs, including models like Llama 2 7B.\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- torch\n",
    "- pynvml\n",
    "\n",
    "Install dependencies:\n",
    "    pip install transformers torch pynvml\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetPowerUsage, nvmlDeviceGetMemoryInfo\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Initialize NVIDIA Management Library\n",
    "nvmlInit()\n",
    "gpu_handle = nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "# Helper functions\n",
    "def track_power() -> float:\n",
    "    \"\"\"Track GPU power consumption in watts.\"\"\"\n",
    "    return nvmlDeviceGetPowerUsage(gpu_handle) / 1000\n",
    "\n",
    "def track_memory() -> float:\n",
    "    \"\"\"Track GPU memory usage in GB.\"\"\"\n",
    "    mem_info = nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    return mem_info.used / (1024 ** 3)\n",
    "\n",
    "def count_model_parameters(model: torch.nn.Module) -> int:\n",
    "    \"\"\"Count the total number of parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "class LLMPerformanceTester:\n",
    "    \"\"\"\n",
    "    A utility class for evaluating the performance of Hugging Face LLMs.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): The Hugging Face model name to load.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        model: The LLM model loaded from Hugging Face.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        parameter_count (int): Number of parameters in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.parameter_count = count_model_parameters(self.model)\n",
    "        print(f\"Model Size: {self.parameter_count:,} parameters\")\n",
    "\n",
    "    def evaluate_latency_throughput(self, prompt: str, max_tokens: int = 50, batch_size: int = 1) -> Tuple[torch.Tensor, float, float, float]:\n",
    "        \"\"\"\n",
    "        Evaluate latency, token throughput, and token processing rate.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt for the model.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "            batch_size (int): Number of prompts in a batch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, float, float, float]: Generated outputs, latency, throughput, and token throughput.\n",
    "        \"\"\"\n",
    "        inputs = [prompt] * batch_size\n",
    "        tokenized_inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "\n",
    "        # Warm-up\n",
    "        print(\"Warming up...\")\n",
    "        start_warmup = time.time()\n",
    "        self.model.generate(**tokenized_inputs, max_new_tokens=max_tokens)\n",
    "        end_warmup = time.time()\n",
    "        warmup_time = end_warmup - start_warmup\n",
    "        print(f\"Warm-up Time: {warmup_time:.2f}s\")\n",
    "\n",
    "        # Measure latency and throughput\n",
    "        print(\"Measuring latency and throughput...\")\n",
    "        start_time = time.time()\n",
    "        outputs = self.model.generate(**tokenized_inputs, max_new_tokens=max_tokens)\n",
    "        end_time = time.time()\n",
    "\n",
    "        latency = end_time - start_time\n",
    "        throughput = batch_size / latency\n",
    "        total_tokens = max_tokens * batch_size\n",
    "        token_throughput = total_tokens / latency\n",
    "        print(f\"Latency: {latency:.2f}s | Throughput: {throughput:.2f} responses/sec | Token Throughput: {token_throughput:.2f} tokens/sec\")\n",
    "        return outputs, latency, throughput, token_throughput\n",
    "\n",
    "    def evaluate_power_efficiency(self, prompt: str, max_tokens: int = 50, batch_size: int = 1) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Evaluate power consumption and efficiency per token.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt for the model.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "            batch_size (int): Number of prompts in a batch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Total power consumed and energy per token.\n",
    "        \"\"\"\n",
    "        inputs = [prompt] * batch_size\n",
    "        tokenized_inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "\n",
    "        # Warm-up\n",
    "        print(\"Warming up...\")\n",
    "        self.model.generate(**tokenized_inputs, max_new_tokens=max_tokens)\n",
    "\n",
    "        # Measure power and efficiency\n",
    "        print(\"Measuring power efficiency...\")\n",
    "        power_start = track_power()\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.generate(**tokenized_inputs, max_new_tokens=max_tokens)\n",
    "\n",
    "        end_time = time.time()\n",
    "        power_end = track_power()\n",
    "\n",
    "        latency = end_time - start_time\n",
    "        throughput = batch_size / latency\n",
    "        power_consumed = (power_end - power_start) * latency\n",
    "        total_tokens = max_tokens * batch_size\n",
    "        energy_per_token = power_consumed / total_tokens if total_tokens > 0 else float('inf')\n",
    "\n",
    "        print(f\"Power Consumption: {power_consumed:.2f} W | Energy per Token: {energy_per_token:.4f} W/token\")\n",
    "        return power_consumed, energy_per_token\n",
    "\n",
    "    def compare_precision_accuracy(self, prompt: str, max_tokens: int = 50) -> bool:\n",
    "        \"\"\"\n",
    "        Compare outputs for fp32 and fp16 precision to test integrity.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt for the model.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether the outputs match between fp32 and fp16 precisions.\n",
    "        \"\"\"\n",
    "        # Full precision (fp32)\n",
    "        model_fp32 = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        output_fp32 = model_fp32.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        text_fp32 = self.tokenizer.decode(output_fp32[0], skip_special_tokens=True)\n",
    "\n",
    "        # Mixed precision (fp16)\n",
    "        model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=torch.float16).to(self.device)\n",
    "        output_fp16 = model_fp16.generate(**inputs, max_new_tokens=max_tokens)\n",
    "        text_fp16 = self.tokenizer.decode(output_fp16[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Full Precision (fp32) Output:\\n{text_fp32}\")\n",
    "        print(f\"Mixed Precision (fp16) Output:\\n{text_fp16}\")\n",
    "\n",
    "        similarity = text_fp32 == text_fp16\n",
    "        print(f\"Outputs Match: {similarity}\")\n",
    "        return similarity\n",
    "\n",
    "    def memory_by_sequence_length(self, base_prompt: str, max_tokens: int = 50, max_length: int = 1024) -> None:\n",
    "        \"\"\"\n",
    "        Evaluate memory usage as sequence length increases.\n",
    "\n",
    "        Args:\n",
    "            base_prompt (str): Base string to repeat for increasing sequence length.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "            max_length (int): Maximum sequence length to test.\n",
    "        \"\"\"\n",
    "        print(\"Memory Usage by Sequence Length:\")\n",
    "        for seq_length in [128, 256, 512, max_length]:\n",
    "            prompt = base_prompt * (seq_length // len(base_prompt))\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            self.model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "            print(f\"Sequence Length: {seq_length} | Peak Memory Usage: {peak_memory:.2f} MB\")\n",
    "\n",
    "\"\"\"\n",
    "Usage example:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"meta-llama/Llama-2-7b-hf\"  # Example model\n",
    "    prompt = \"Explain the impact of climate change on global agriculture.\"\n",
    "    base_prompt = \"Climate change affects agriculture in multiple ways. \"\n",
    "\n",
    "    tester = LLMPerformanceTester(model_name)\n",
    "\n",
    "    # Latency and token throughput\n",
    "    tester.evaluate_latency_throughput(prompt, max_tokens=50, batch_size=4)\n",
    "\n",
    "    # Power efficiency and energy per token\n",
    "    tester.evaluate_power_efficiency(prompt, max_tokens=50, batch_size=4)\n",
    "\n",
    "    # Compare fp32 and fp16 outputs\n",
    "    tester.compare_precision_accuracy(prompt, max_tokens=50)\n",
    "\n",
    "    # Memory by sequence length\n",
    "    tester.memory_by_sequence_length(base_prompt, max_tokens=50, max_length=1024)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d1aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "metrics.py\n",
    "\n",
    "This module contains implementations and utilities to evaluate language models (LLMs) based on various\n",
    "metrics, including BLEU, ROUGE, METEOR, BERTScore, and others.\n",
    "\n",
    "The metrics are demonstrated with the Hugging Face model \"Mistral\" as an example.\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- torch\n",
    "- datasets\n",
    "- bert_score\n",
    "- nltk\n",
    "\n",
    "Install dependencies:\n",
    "    pip install transformers torch datasets bert-score nltk\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Example Hugging Face model for evaluation\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "def calculate_bleu(predictions: List[str], references: List[List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[List[str]]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: BLEU score and additional metrics.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/huggingface/evaluate\n",
    "    \"\"\"\n",
    "    bleu_metric = load_metric(\"bleu\")\n",
    "    bleu_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = bleu_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_rouge(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: ROUGE scores.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/huggingface/evaluate\n",
    "    \"\"\"\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    rouge_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = rouge_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_meteor(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate METEOR score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: METEOR score.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/huggingface/evaluate\n",
    "    \"\"\"\n",
    "    meteor_metric = load_metric(\"meteor\")\n",
    "    meteor_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = meteor_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_bert_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate BERTScore for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Precision, Recall, and F1 scores.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/Tiiiger/bert_score\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(predictions, references, lang=\"en\", verbose=True)\n",
    "    return {\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()}\n",
    "\n",
    "def calculate_ragas_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate RAGAS (Retrieval-Augmented Generation Answer Score).\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: RAGAS score.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/explodinggradients/ragas\n",
    "    \"\"\"\n",
    "    from ragas import evaluate\n",
    "    ragas_result = evaluate(predictions, references)\n",
    "    return {\"ragas_score\": ragas_result[\"score\"]}\n",
    "\n",
    "def calculate_helm_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate HELM (Holistic Evaluation of Language Models) metrics.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: HELM score.\n",
    "\n",
    "    Resource:\n",
    "        https://crfm.stanford.edu/helm/latest/\n",
    "    \"\"\"\n",
    "    # Placeholder for actual HELM evaluation framework integration\n",
    "    helm_score = np.mean([len(pred) / max(len(ref), 1) for pred, ref in zip(predictions, references)])\n",
    "    return {\"helm_score\": helm_score}\n",
    "\n",
    "def calculate_gpt_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate GPT-Score for text similarity.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: GPT-Score values.\n",
    "\n",
    "    Resource:\n",
    "        https://github.com/IntelLabs/gpt-score\n",
    "    \"\"\"\n",
    "    from gpt_score import GPTScorer\n",
    "    scorer = GPTScorer()\n",
    "    scores = scorer.score(predictions, references)\n",
    "    return {\"gpt_score\": np.mean(scores)}\n",
    "\n",
    "def calculate_forgetting_rate(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, prompts: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Forgetting Rate of the model over repeated evaluations.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): The language model to test.\n",
    "        tokenizer (AutoTokenizer): Tokenizer associated with the model.\n",
    "        prompts (List[str]): List of input prompts.\n",
    "\n",
    "    Returns:\n",
    "        float: Forgetting rate as a percentage.\n",
    "\n",
    "    Resource:\n",
    "        https://arxiv.org/abs/2205.12647\n",
    "    \"\"\"\n",
    "    baseline_results = []\n",
    "    repeated_results = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        baseline_output = model.generate(**inputs)\n",
    "        repeated_output = model.generate(**inputs)\n",
    "\n",
    "        baseline_results.append(tokenizer.decode(baseline_output[0], skip_special_tokens=True))\n",
    "        repeated_results.append(tokenizer.decode(repeated_output[0], skip_special_tokens=True))\n",
    "\n",
    "    differences = [1 if b != r else 0 for b, r in zip(baseline_results, repeated_results)]\n",
    "    forgetting_rate = sum(differences) / len(prompts) * 100\n",
    "    return forgetting_rate\n",
    "\n",
    "def calculate_brevity_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate Brevity Score to evaluate concise text generation.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Brevity score.\n",
    "\n",
    "    Resource:\n",
    "        https://arxiv.org/pdf/1904.09675.pdf\n",
    "    \"\"\"\n",
    "    brevity_ratios = [len(pred.split()) / max(len(ref.split()), 1) for pred, ref in zip(predictions, references)]\n",
    "    brevity_score = np.mean([min(1.0, ratio) for ratio in brevity_ratios])\n",
    "    return {\"brevity_score\": brevity_score}\n",
    "\n",
    "\"\"\"\n",
    "Usage Example with Hugging Face LLM Mistral:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Sample inputs\n",
    "    predictions = [\"Climate change is a global challenge that requires...\"]\n",
    "    references = [[\"Climate change is a pressing issue affecting...\"]]\n",
    "\n",
    "    # BLEU\n",
    "    print(\"BLEU Score:\", calculate_bleu(predictions, references))\n",
    "\n",
    "    # ROUGE\n",
    "    print(\"ROUGE Score:\", calculate_rouge(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # METEOR\n",
    "    print(\"METEOR Score:\", calculate_meteor(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # BERTScore\n",
    "    print(\"BERTScore:\", calculate_bert_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # RAGAS\n",
    "    print(\"RAGAS Score:\", calculate_ragas_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # HELM\n",
    "    print(\"HELM Score:\", calculate_helm_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # GPT-Score\n",
    "    print(\"GPT-Score:\", calculate_gpt_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # Forgetting Rate\n",
    "    prompts = [\"What is climate change?\", \"Explain photosynthesis.\"]\n",
    "    print(\"Forgetting Rate:\", calculate_forgetting_rate(model, tokenizer, prompts))\n",
    "\n",
    "    # Brevity Score\n",
    "    print(\"Brevity Score:\", calculate_brevity_score(predictions, [r[0] for r in references]))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863485d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def calculate_perplexity(probabilities: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model's output.\n",
    "\n",
    "    Args:\n",
    "        probabilities (List[float]): A list of probabilities for each token in the sequence.\n",
    "\n",
    "    Returns:\n",
    "        float: The perplexity score.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/perplexity\n",
    "    \"\"\"\n",
    "    cross_entropy = -np.mean(np.log(probabilities))\n",
    "    perplexity = np.exp(cross_entropy)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision: float, recall: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the F1 score given precision and recall.\n",
    "\n",
    "    Args:\n",
    "        precision (float): Precision of the predictions.\n",
    "        recall (float): Recall of the predictions.\n",
    "\n",
    "    Returns:\n",
    "        float: The F1 score.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/f1\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def calculate_precision_recall(true_positive: int, false_positive: int, false_negative: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate precision and recall.\n",
    "\n",
    "    Args:\n",
    "        true_positive (int): Number of true positive cases.\n",
    "        false_positive (int): Number of false positive cases.\n",
    "        false_negative (int): Number of false negative cases.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing precision and recall scores.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/precision\n",
    "    \"\"\"\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "    return {\"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "def calculate_mean_reciprocal_rank(ranks: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Reciprocal Rank (MRR).\n",
    "\n",
    "    Args:\n",
    "        ranks (List[int]): A list of ranks for the first relevant result in each query.\n",
    "\n",
    "    Returns:\n",
    "        float: The MRR score.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/mrr\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = [1 / rank if rank > 0 else 0 for rank in ranks]\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "\n",
    "def calculate_mean_average_precision(relevance_scores: List[List[int]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision (MAP).\n",
    "\n",
    "    Args:\n",
    "        relevance_scores (List[List[int]]): A list of binary relevance scores for each query's retrieved documents.\n",
    "\n",
    "    Returns:\n",
    "        float: The MAP score.\n",
    "\n",
    "    Reference:\n",
    "        - https://huggingface.co/docs/evaluate/metrics/map\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    for scores in relevance_scores:\n",
    "        precision_at_k = [\n",
    "            sum(scores[:k + 1]) / (k + 1) for k in range(len(scores)) if scores[k] == 1\n",
    "        ]\n",
    "        if precision_at_k:\n",
    "            average_precisions.append(np.mean(precision_at_k))\n",
    "    return np.mean(average_precisions) if average_precisions else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb99a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa8d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b5415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5137ac17",
   "metadata": {},
   "source": [
    "## Appendix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "metrics.py\n",
    "\n",
    "This module contains implementations and utilities to evaluate language models (LLMs) based on various\n",
    "metrics, including BLEU, ROUGE, METEOR, BERTScore, and others.\n",
    "\n",
    "The metrics are demonstrated with the Hugging Face model \"Mistral\" as an example.\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- torch\n",
    "- datasets\n",
    "- bert_score\n",
    "- nltk\n",
    "\n",
    "Install dependencies:\n",
    "    pip install transformers torch datasets bert-score nltk\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from bert_score import score\n",
    "\n",
    "# Example Hugging Face model for evaluation\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "def calculate_bleu(predictions: List[str], references: List[List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[List[str]]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: BLEU score and additional metrics.\n",
    "    \"\"\"\n",
    "    bleu_metric = load_metric(\"bleu\")\n",
    "    bleu_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = bleu_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_rouge(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: ROUGE scores.\n",
    "    \"\"\"\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    rouge_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = rouge_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_meteor(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate METEOR score for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: METEOR score.\n",
    "    \"\"\"\n",
    "    meteor_metric = load_metric(\"meteor\")\n",
    "    meteor_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = meteor_metric.compute()\n",
    "    return result\n",
    "\n",
    "def calculate_bert_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate BERTScore for the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Precision, Recall, and F1 scores.\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(predictions, references, lang=\"en\", verbose=True)\n",
    "    return {\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()}\n",
    "\n",
    "def calculate_ragas_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate RAGAS (Retrieval-Augmented Generation Answer Score).\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder RAGAS scores.\n",
    "    \"\"\"\n",
    "    # Implementation will depend on specific retrieval-augmented scoring methods\n",
    "    return {\"ragas_score\": 0.85}  # Placeholder value\n",
    "\n",
    "def calculate_helm_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate HELM (Holistic Evaluation of Language Models) metrics.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder HELM scores.\n",
    "    \"\"\"\n",
    "    # Implementation requires HELM evaluation framework\n",
    "    return {\"helm_score\": 0.9}  # Placeholder value\n",
    "\n",
    "def calculate_gpt_score(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate GPT-Score.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder GPT-Score.\n",
    "    \"\"\"\n",
    "    return {\"gpt_score\": 0.88}  # Placeholder value\n",
    "\n",
    "def scenario_fidelity_tests(predictions: List[str], scenarios: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function to evaluate Scenario Fidelity Tests.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        scenarios (List[str]): A list of test scenarios.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder scenario fidelity results.\n",
    "    \"\"\"\n",
    "    return {\"scenario_fidelity\": 0.92}  # Placeholder value\n",
    "\n",
    "def calculate_forgetting_rate(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, prompts: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate Forgetting Rate of the model over time.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): The language model to test.\n",
    "        tokenizer (AutoTokenizer): Tokenizer associated with the model.\n",
    "        prompts (List[str]): List of input prompts.\n",
    "\n",
    "    Returns:\n",
    "        float: Placeholder forgetting rate.\n",
    "    \"\"\"\n",
    "    return 0.05  # Placeholder value\n",
    "\n",
    "def rimu_evaluation(predictions: List[str], references: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Placeholder function for RIMU (Relevance, Integration, Memory, Usefulness) evaluation.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted texts from the model.\n",
    "        references (List[str]): A list of reference texts (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Dict: Placeholder RIMU evaluation results.\n",
    "    \"\"\"\n",
    "    return {\"rimu_score\": 0.87}  # Placeholder value\n",
    "\n",
    "def problem_solving_effectiveness(predictions: List[str], problems: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Placeholder function to calculate problem-solving effectiveness of the model.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[str]): A list of predicted solutions from the model.\n",
    "        problems (List[str]): A list of problems to solve.\n",
    "\n",
    "    Returns:\n",
    "        float: Placeholder effectiveness score.\n",
    "    \"\"\"\n",
    "    return 0.91  # Placeholder value\n",
    "\n",
    "\"\"\"\n",
    "Usage Example with Hugging Face LLM Mistral:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Sample inputs\n",
    "    predictions = [\"Climate change is a global challenge that requires...\"]\n",
    "    references = [[\"Climate change is a pressing issue affecting...\"]]\n",
    "\n",
    "    # BLEU\n",
    "    print(\"BLEU Score:\", calculate_bleu(predictions, references))\n",
    "\n",
    "    # ROUGE\n",
    "    print(\"ROUGE Score:\", calculate_rouge(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # METEOR\n",
    "    print(\"METEOR Score:\", calculate_meteor(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # BERTScore\n",
    "    print(\"BERTScore:\", calculate_bert_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # RAGAS\n",
    "    print(\"RAGAS Score:\", calculate_ragas_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # HELM\n",
    "    print(\"HELM Score:\", calculate_helm_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # GPT-Score\n",
    "    print(\"GPT-Score:\", calculate_gpt_score(predictions, [r[0] for r in references]))\n",
    "\n",
    "    # Scenario Fidelity\n",
    "    print(\"Scenario Fidelity:\", scenario_fidelity_tests(predictions, [\"scenario example\"]))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d3e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56eebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
